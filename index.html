<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Locating Energy Infrastructure with Deep Learning </title>

  <!-- Bootstrap core CSS -->
  <link rel="shortcut icon" type="image/png" href="img/icon.png" />

  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/agency.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav text-uppercase my-auto text-center">
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#methodology">Methodology</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#results">Results</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#key-takeaways">Key Takeaways</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#future-work">Future Work</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="masthead">
    <div class="container">
      <div class="intro-text">
        <div class="intro-lead-in">Bass Connections 2020-2021</div>
        <div class="intro-heading text-uppercase">Deep Learning for Rare Energy Infrastructure in Satellite Imagery
        </div>
        <p class="text-white-75 font-weight-light mb-5" style="font-size: 30px">Ada Ye, Eddy Lin, Jose Luis Moscoso, Tyler Feldman, Jessie Ou, Wendy Zhang</p>
        <a class="btn btn-primary btn-xl js-scroll-trigger" href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">See Our
          Github Repository</a>
        <a class="btn btn-primary btn-alt js-scroll-trigger" href="https://figshare.com/articles/dataset/Baseline_Dataset_and_Synthetic_Images/13626377"> Dataset </a>
      </div>
    </div>
  </header>


  <!-- Motivation -->
  <section class="page-section" id="motivation">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Motivation</h2>

          <hr class="divider light my-4" />
        </div>
      </div>
      <div class="row justify-content-center">
        <div class="row">
        <div class="col-md-12">
          <h3>Energy Access Planning</h3>
          <p class="text-muted">Access to electricity is correlated with improvements in income, education, maternal mortality, and gender
            equality. Around <a href="https://energyeducation.ca/encyclopedia/Access_to_electricity">1.2 billion people worldwide do not
            have electricity in their homes</a>, many of them located in Subsaharan Africa and Asia. Information on the location and
            characteristics of energy infrastructure comprising generation, transmission, and end-use consumption can inform policy makers
            in energy access planning and is critical to efficiently deploying energy resources. Such information will allow energy developers
            to understand how best serve the electricity needs of communities without access to electricity through either power grid
            extensions, micro/mini grids, or off-grid solutions.</p>
          </div>
         </div>
        <div class="row">
        <div class="col-md-12">
          <img src="img/motivation/Global Access to Electricity.png" class="img-responsive" alt=""style="width:100%; height:70%">
          <caption><i>Image from: <a href="https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/">
            https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/</a></i></caption>

            <p style="margin-top: 2.5em" class="text-muted"  style="text-align: justify">However, energy data for developers is often outdated,
              incomplete, or inaccessible. This is a common issue for NGO’s and other developers of distributed energy resources (DER) and
              microgrids who are looking to provide additional electricity access. </p>
            <p class="text-muted"  style="text-align: justify"> One potential solution to this lack of energy infrastructure data is to automate
              the process of mapping energy infrastructure using deep learning in satellite imagery. Using deep learning, we can feed an overhead
              image to a model and make predictions about the contents or characteristics of the region photographed in the image. Using this
              tool, the resulting information on energy infrastructure can then help inform energy access planning, such as deciding the most
              cost-effective option among distributed generation, micro-grids, or grid extension strategies for electrification.</p>
          </div>
        </div>
    </div>


        <!--<div class="col-md-6">
          <h3 class="service-heading">Deep Learning</h3>
          <p class="text-muted" style="text-align: justify;">
            Using deep learning, we can feed an image to a model, and the model is able to make predictions about the contents or characteristics of that image. A common
            technique for image analysis is classification, in which the model predicts the class of the image out of a list of possible classes.
            In the image on the right, the model predicts that the input picture is of a cat, and is 98% confident with this classification. The model learns how to predict
            these classifications based on examples that are shown to it. After it has been trained, it can classify images that it has not seen before.
          </p>
        </div>

        <div class="col-md-6">
          <img src="img/motivation/cat_image_classifier.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Images from: <a href="https://www.codeproject.com/Articles/5160467/Image-Classification-Using-Neural-Networks-in-NET">
            Code Project</a></i></caption>
        </div>-->


     <div class="row text-center">
      <h3 class="service-heading">Object Detection</h3>
        <div class="row">

          <div class="col-md-6">
          <img src="img/motivation/object detection explanation.jpeg" class="img-responsive" alt="" style="width:90%">
          <caption><i>Image from: <a href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">
            You only look once, or YOLO</a></i></caption>
          </div>


        <div class="col-md-6">
          <p class="text-muted" style="text-align: justify;">
            For this project, we focus on object detection, which is a combination of classification with object localization.
            The model analyzes images and predicts bounding boxes that surround each object within the image. It  also classifies
            each object, producing a confidence score corresponding to the level of certainty of its prediction. In the image on
            the left, the model predicted that there were different objects represented by each of the boxes shown in green, yellow
            and pink. The model also predicted that the objects within these colored boxes were a handbag, a car, and a person
            respectively. The model learns how to predict these boxes and classifications based on examples shown to it. These
            examples have labels (the object’s class and the location of the bounding box within the image) that we collectively
            refer to as ground truth.
          </p>
        </div>
       </div>
      </div>


      <div class="row text-center">
      <h3 class="service-heading">Applying Deep Learning to Overhead Imagery</h5>
          <div class="row">

            <div class="col-md-12 my-auto">
              <img src="img/motivation/apply deep learning model to overhead imagery.png" class="img-responsive" alt="apply deep learning" style="width:90%">
            </div>
          </div>

          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                 After training this model, we could apply it to a collection of overhead imagery to locate and classify energy
                 infrastructure across a whole region. While we could demonstrate this for any number of types of electricity
                 infrastructure, we use wind turbines because they are relatively homogeneous in appearance, unlike power plants,
                 for example, which come in many different configurations. Outside of size differences, there is little variation
                 between types of wind turbines. We also start with data from within the US, because high resolution overhead imagery
                 data are readily available across the continental US.</p>
             </div>
          </div>
        </div>


			<div class="row text-center">
			  <h3 class="service-heading">Challenges with Object Detection</h3>

			  <div class="row">
  			  <div class="col-md-6">
              <h4>Problem 1: Lack of labeled data for rare objects</h4>
              <p class="text-muted" style="text-align: justify;"> Object detection networks, like the one used in this work, are
                notorious for their "data hunger," requiring <strong>large amounts of annotated training data to perform well</strong>.
                For common infrastructure like buildings and roads, there is ample real-world data available to train such models.
                However, energy infrastructure is <strong>rare</strong> both in number and density, so collecting and annotating large
                amounts of satellite images manually is <strong>expensive</strong> and time consuming.
              </p>
          </div>

          <div class="col-md-6">
              <img src="img/motivation/wind_turbines_rarity.png" class="img-responsive" alt=""style="width:80%; height:80%">
          </div>
        </div>

        <div class="row">
          <div class="col-md-6">
            <h4>Problem 2: Domain adaptation</h4>
            <p class="text-muted" style="text-align: justify;">
              Typically our labeled training data are not from the same location as where we want to apply these techniques. However, object
              detection models poor performance on images that are from domians that are <strong>not similar</strong> to the ones that it
              has previously seen. In our case, this means the model will struggle when trying to apply it to <strong>new geographies</strong>
              and to <strong>different variations</strong> of styles of energy infrastructure.
            </p>
          </div>

        <div class="col-md-6">
          <img src="img/motivation/domain_adaptation_updated.png" class="img-responsive" alt=""style="width:80%; height:80%">
          </div>
        </div>
    </div>

     <div class="row text-center">
      <h3 class="service-heading">Proposed solution: synthetic imagery</h3>
        <div class="row">
           <div class="col-md-12 my-auto">

            <p class="text-muted" style="text-align: justify;">
              Since training data are difficult to collect, in this project we explore creating synthetic data to supplement the real data
              that are available. We do this by taking a real image without any energy infrastructure present and introducing a 3D model of
              an object of interest on top of that image as seen in the figure below. To add these 3D models to real images we use CityEngine,
              which can render and generate 3D landscapes and structures based on input from the designer. In our case, we populate a landscape
              with synthetic wind turbines. Then we position the camera in the overhead position and capture images in a manner that mimics
              the appearance of overhead imagery. Knowing where we placed the wind turbines in the synthetic image, we also generate ground
              truth labels for each of these images.
            </p>
        </div>
        <div class="col-md-12">
          <img src="img/motivation/synthetic imagery.png" class="img-responsive" alt=""style="width:90%; height:90%">
        </div>
      </div>
    </div>




      <div class="row text-center">
        <div class="col-md-6">
          <h3 class="service-heading">Previous work</h3>
          <p class="text-muted" style="text-align: justify;">
            For five years, the Duke Energy Data Analytics Lab has worked on developing deep learning models that identify energy infrastructure,
            with an end goal of generating maps of power systems and their characteristics that can aid policymakers in implementing effective
            electrification strategies. In 2015-16, researchers created a model that can detect solar photovoltaic arrays with high accuracy <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-2015-2016">[2015-16
                Bass Connections Team]</a>.
            In 2018-19, this model was improved to identify different types of transmission and distribution energy infrastructures,
            including power lines and transmission towers <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-energy-infrastructure-map-world-through-satellite-data-2018">[2018-19
                Bass Connections Team]</a>. Last year's project focused on increasing the adaptability of detection models
            across different geographies by creating realistic synthetic imagery <a
                href="https://bass-connections-2019.github.io/">[2019-20
            Bass Connections Team]</a>. In our project, we build upon this progress and try to improve the model's ability to detect rare objects
            in new, diverse locations.
          </p>
        </div>
        <div class="col-md-6">
          <img src="img/overview_OLD/energy_information.png" class="img-responsive center" alt=""
          style="width:100%">
        </div>
      </div>


		</div>
	</section>

  <!-- Methodology -->
  <section class="page-section" id="methodology">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Methodology</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                Below you can see a summary of the experiments we ran to explore whether adding synthetic imagery
                improves the performance of an object detection model across geographic domains. We first need to collect
                real and synthetic imagery, and then we can create two datasets. One dataset contains purely real imagery,
                while the second contains the same real imagery, but we add in some synthetic images. We can train
                an object detection model on the first dataset, test its performance, and then do the same with the second
                dataset, and finally compare the two performances. If the performance of the model is better when trained on
                the dataset that had added synthetic imagery, we can say that the synthetic imagery helps the performance of
                the model. In this section, we'll walk through each of the steps required to perform this experiment.
              </p>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Overview of Steps.png" class="img-responsive" alt=""style="width:100%">
            </div>
          </div>
          <hr class="divider light my-4" />
          <h2 class="service-heading text-left">Collecting Real Imagery</h2>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> For our overhead imagery of wind turbines, we chose to sample them from
                <a href="https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/">the National Agriculture Imagery Program</a>.
                This imagery covers a large part of the US and is very high resolution, making it great for our experiments.
                We collected imagery in three different regions that we called Northwest, Northeast, and Eastern Midwest. We noticed
                differences in the visual appearance of the background in the images collected in these three regions:
              </p>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly brown</dd>
                <dd class="text-muted">- Mostly desert and grassland</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northeast</h5>
              <dl>
                <dd class="text-muted">- Hue is very green</dd>
                <dd class="text-muted">- Mostly forests</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Eastern Midwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly green, some brown</dd>
                <dd class="text-muted">- Primarily farmland</dd>
              </dl>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Collecting Overhead Imagery.png" class="img-responsive" alt="" style="width:80%"><br>
              <caption><i>Figure: Each dot represents a single image that we collected.</i></caption>
            </div>
          </div>
          <br>
          <br>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> Below we can see the regions splits by which states they include,
                as well as the number of images we collected for each region.
              </p>
              <img src="img/methodology/Where Our Data is From.png" class="img-responsive" alt="" style="width:70%"><br>
              <caption><i>Figure: Map of the U.S. showing the U.S. states and number of images we collected in each region</i></caption>
            </div>
          </div>
        </div>
      </div>


      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Creating Synthetic Imagery</h2>
          <p class="text-muted" style="text-align: left;"> To create synthetic imagery, we use a software called CityEngine. As inputs to
            this process, we need a list of background images that do not contain any wind turbines and a 3D model of a wind turbine. We then can automatically generate synthetic images.
            First, the software places a randomly chosen background image and then randomly generates 3D wind turbine models
            on top of the background image to create the 3D scene shown in the middle of the figure below. Next, the software moves a simulated camera
            in the overhead/bird's eye view and saves these overhead images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Synthetic Generation Process.png" class="img-responsive" alt="" style="width:100%">
        </div>
      </div>

      <br><br>
      <div class="row">
        <div class="col-md-4 my-auto">
          <p class="text-muted" style="text-align: justify;"> We can repeat this process but remove the background images, and color
            in the turbine models as black to retrieve information on where the turbine models are located. The black pixels in these
            images can be automatically grouped together to locate the wind turbines and create a formatted label that contains the
            bounding box around each turbine model.
          </p>
        </div>
        <div class="col-md-8 my-auto">
          <img src="img/methodology/Generating Annotations.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Figure: Side-by-side of an RGB image with its corresponding black-and-white label.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: justify;"> These synthetic images are simple, cheap, and fast to create. All we need are
            images for the background, and unlabeled imagery is far easier to acquire than labeled imagery. The rest of the process of generating the images and the labels is done automatically, making this a great
            alternative when we do not have enough real imagery or when it is time-consuming or expensive to collect. Below are some example
            synthetic images created from a variety of background images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Example Synthetic Images.png" class="img-responsive" alt="" style="width:80%;"><br>
          <caption class="text-muted"><i>Figure: Our synthetic images contain a variety of background images and camera angles. They look fairly similar to our real imagery.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <h3 class="service-heading">Synthetic Imagery Design Considerations</h3>
          <p class="text-muted" style="text-align: left;"> The design of the synthetic imagery is important because the closer the
            synthetic imagery is to the real imagery, the more the synthetic imagery will improve our performance when adding it to our
            training set.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Turbine Size Distribution.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Size of the Synthetic Turbines</h5>
          <p class="text-muted" style="text-align: left;"> The first consideration we have to make is what size to make the
            synthetic turbines. For this issue, we chose to model the size distribution of the synthetic turbines after the size
            distribution of the real turbines. We created a histogram of the size of the turbines in our real imagery,
            and modeled this in our synthetic imagery with multiple bins of uniform distributions.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Angle of the Camera</h5>
          <p class="text-muted" style="text-align: left;"> The next decision we had to make about the synthetic imagery design was
            the angle of the simulated camera when we are capturing photos. We noticed that in the real imagery,
            some of the images were captured at an angle. In the real overhead imagery, you can see the pole of
            the turbine due to the angle of the camera. We observed that about half of the real images were taken from directly above (90 degrees),
            and the rest were taken at a variety of angles that were between 60-90 degrees. In our synthetic image generation process, we
            chose half of the time to take the image from directly above. The other half of the time, we would use a
            randomly chosen camera angle between 60 and 90 degrees.
          </p>
        </div>
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Angle of Camera 2.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Nearby Background Image.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Test image and nearby collected background image.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Which Background Images to Use</h5>
          <p class="text-muted" style="text-align: left;"> We also have to choose which background images to have placed under our
            synthetic wind turbine models. We chose to collect background imagery close to the real images in our testing set so that our synthetic
            imagery would look as close as possible to the target data. We may likely be able to collect unlabelled imagery from around the region we wish to test
            on for use as background images. Using the background images close to our testing locations allows us to estimate the potential performance
            increase that the synthetic data can provide without adding in confounding variables such as a mismatch between the synthetic background
            image domain and the target domain.
          </p>
        </div>
      </div>

      <hr class="divider light my-4" />

      <div class="row">
        <div class="col-md-12 my-auto">
        <h2 class="service-heading">Constructing the Datasets</h2>

        <div class="row">
          <div class="col-md-12 my-auto">
            <h5 class="service-heading">Clustering and Stratified Sampling</h5>
              <p class="text-muted" style="text-align: justify;">
                Because our self-defined geographic regions are wide and diverse, it's important for our training and testing datasets
                to be representative of a given geographic region. To increase homogeneity within a region (we define a region as a "domain"), we clustered 
                within each region and then performed stratified sampling from each cluster to construct our 1:1 train:validation ratio (1 training image for each validation image) constrained
                baseline datasets, including 100 images total across the clusters within each domain. Each point represents an image, and each color represents a cluster. Points that are in the same cluster are more similar to each other than points that are in different clusters. </p>
          </div>
          <div class="col-md-12 my-auto">
            <p style="text-align:center;">
            <img src="img/methodology/Clustering and Stratified Sampling.png" class="img-responsive" alt="" class="center" style="width:80%; height:70%">
            </p>
            <br>
            <caption class="text-muted"><i>Figure: DBSCAN clustering within each geographic region and then stratified random sampling from each cluster to construct our training and testing datasets. Each dot represents a real imge, and each color represents a different strata or cluster. Because our training and testing datasets contain 100 images each, and there are 4 clusters within the Northeast region shown here, we randomly sample 25 images from each cluster for our train data, and a different set of 25 images from each cluster for our test data. </i></caption>
          </div>
        </div>

          <h5 class="service-heading">Optimizing the Ratio of Real to Synthetic Data</h5>
          <div class="row">

            <div class="col-md-6 my-auto">
              <img src="img/methodology/Ratio Test Experimental Design.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
            </div>

            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;">
                To construct our baseline and add synthetic data, we need to figure out what ratio of real to synthetic data yields
                the largest gain in performance (if any). If we add too much synthetic data, then we run the risk of overfitting to synthetic data. If we add too little synthetic data, then it will have little impact on performance. To find this ratio, we designed an experiment, where we test ratios of 1:0, 1:0.5, 1:0.75,
                1:1, and 1:2 real to synthetic ratios. After conducting these experiments, we found that 1:0.75 yields the greatest performance
                as measured by average precision. Therefore, we design our experiments using the 1:0.75 ratio.  </p>
             </div>
        </div>

      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Experimental Setup</h2>
          <br>
          <p class="text-muted" style="text-align: justify;">
            Having clustered and sampled our data and found the optimal real to synthetic ratio, our final datasets for each region is: </p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Baseline: </b> Train on 100 Real Non-Target Images, Test on 100 Target Domain Images</li>
                <li class="text-muted" style="text-align: justify;"><b>Modified: </b> Train on 100 Real Non-Target Images + 75 Syn Target Images, Test on 100 Target Domain Images </li>
              </ul>
          </br>

          <h5 class="service-heading">Overview</h5>
          <div class="row">
            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;">In the context of our work, we define a <strong>domain</strong> as a geographic region, such as the US Pacific Northwest. A <strong>target domain</strong>  is where we aim to apply our model to, whereas a <strong>source domain</strong> is where the real training data comes from. 
              
              The long-term goal of our project is to apply our object detection model to a variety of unseen domains around the world, especially in developing areas where information on energy infrastructure is hard to obtain. This requires our model to be able to <strong>generalizable</strong>,
                that is, the ability to perform well on images that are different from the images it was trained on. One real-world application in energy access planning is a scenario where there is limited labeled real data in target locations where we want to deploy energy resources to. For this scenario, we design <strong>within-domain</strong> experiments, where the target domain remains the same geographic region as source domain. 
                
                A second and more challenging scenario is when there is no real data in the target locations where we want to deploy energy resources to. Because the target domain has no real data, we have to draw data from an alternative source domain. For this scenario, we design <strong>cross-domain</strong> experiments, where we train on a source domain but we test on a target domain. 
                
               </p>
             </div>
           
            <div class="col-md-6 my-auto">
              <img src="img/methodology/Overall Domain Map.png" class="img-responsive" class="center" alt="" style="width:100%">
              <br>
              <caption class="text-muted"><i>Figure: Overall Experiment Setup. In within-domain experiments, the target domain (Northwest) remains the same geographic region as the source domain (Northwest). In cross-domain experiments, the target domain (Northeast) has no labeled real data, so the model is trained on a different source domain (Northwest) and then applied to the target domain. Orange color denotes a source domain, whereas blue color denotes a target domain.</i></caption>
            </div>
            </div>
          
          <div class="row">
           <div class="col-md-6 my-auto">
             <img src="img/methodology/Pairwise Cycle.png" class="img-responsive" alt="Pairwise Experiment Setup" style="width:100%">
              <caption class="text-muted"><i>Figure: Pairwise experiment setup. The arrow tails point toward the source domain (where real training data comes from), whereas the arrow heads point toward the target domain (where the model will be tested on). Bi-directional arrows indicate each domain serves as the source for testing the other two domains, and in a separate experiment, the same domain serves as the target to be tested using model trained on the other domains. </i></caption>
            </div>

            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;"> Our hypothesis is that synthetic
                imagery will help our model generalize to unseen domain - the model performance will improve when testing on a new geography in the presence
                of synthetic data. We assume that training and testing on separate data, but from within the same domain, will perform better in general than training and testing on separate data from different domains. Note that although examples from continental US is shown here, in reality, a target domain is likely a less developed area without reliable access to electricity, and a source domain is likely a developed area with readily available data on energy infrastructures.  </p>
             <p class="text-muted" style="text-align: justify;">
               In our case, each domain is a geographic region that we defined. Namely, Northwest, Northeast, and Eastern Midwest. To test the impact of each region, we run a 3-way pairwise experiment, where we train and test on each of the 3 regions, resulting in 6 experiments. For example, the Northeast region serves as the source domain for testing Northwest and Eastern Midwest. Then in a different experiment, a model trained using Eastern Midwest is tested on Northeast, and in another experiment, a model trained on Northwest is tested on Northeast. </p>
              
            </div>
          </div>


          <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Within Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: <strong>Within-domain</strong> experiment example using Northwest as target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Within Domain Experiment (Target = Source)</h5>
             <p class="text-muted" style="text-align: left;">
               Before we test how well synthetic imagery improves performance when the model is testing on a different domain than the one
               it was trained on, we need to establish some baseline performance results for detecting wind turbines in the imagery to evaluate whether adding synthetic imagery improves our model's accuracy. We need to know how well the model is doing when it is trained and
               tested on the same domain. Using Northwest region as our target domain, we ran our baseline and modified experiments,
               and we repeat for the other domains/regions.</p>
          </div>
        </div>
        <br>


        <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Cross Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: <strong>Cross-domain</strong> experiment example using Northwest as target domain and Eastern Midwest as non-target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Cross Domain Experiment (Target not equal to Source)</h5>
             <p class="text-muted" style="text-align: left;">
               These experiments are designed to test if synthetic data can help overcome differences in geography. To test whether adding synthetic imagery improves our model's generalizability, we trained on one region, and tested
               on another region (cross-domain). Then we added synthetic imagery similar to the target domain and compared performance against the baseline. Augmenting our baseline dataset with synthetic data not only provides more exmaples of images with wind turbines, but it provides examples that are more similar to the target locations the model will be deployed to. Thus, we expect adding synthetic data to improve our model's generalizability. 
               Here we demonstrate this experiment setup using Northwest as the target domain and Eastern Midwest as the non-target domain.
               We repeated for the other domains as well. </p>
          </div>
        </div>
        <br>

       </div>
      </div>

    </div>
  </section>

  <!-- Results-->
  <section class="page-section" id="results">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Results</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-6 my-auto">
              <br>
              <img src="img/results/metrics.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">
                Figure 1: The model predicted that 4 objects were wind turbines. 2 of those predictions were correct, meaning
                the precision would be 2/4. There are 3 wind turbines in the image and the model found 2 of these, meaning the
                recall would be 2/3.</font></caption>
            </div>
            <div class = "col-md-6">
              <h3>Performance Metrics</h3>
              <p class="text-muted" style="text-align: justify;">
                To understand our results, it's important that we first understand the metrics that we have chosen to measure
                performance. The primary metrics we will use is Average Precision. We will explain
                the implication of this metric starting with the images on the left.</p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Precision: </b> Out of the objects that the model classified as a wind turbine,
                  what fraction of these were actually wind turbines.</li>
                <li class="text-muted" style="text-align: justify;"><b>Recall: </b> Out of wind turbines present in the data, what fraction of these
                  did the model find.</li>
              </ul>
            </div>
            <div class="row">
              <div class = "col-md-6 my-auto">
                <p class="text-muted" style="text-align: justify;">
                  Now we plot the values of precision and recall a model can reach when doing predictions on a graph, which is known as a precision-recall
                  curve. You can see on the curve that as precision increases, recall decreases, and vice versa. There is hence a tradeoff between
                  precision and recall. However, we would like to have high values for both precision and recall, which means we would like the area
                  under the precision-recall curve to be as high as possible. A metric that quantifies this area is Average Precision (AP). </p>
                <p class="text-muted" style="text-align: justify;">
                  Note that in the machine learning space, a small absolute increase in AP is already a significant improvement.
                </p>
              </div>
              <div class="col-md-6">
                <br>
                <img src="img/results/sample_pr.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 2: Sample Precision Recall Curve. We would like the curve to move to right as much as possible as indicated
                  by the green arrow. </font></caption>
                <br>
                <br>
                <br>
              </div>
              <div class="row">
                <div class="col-md-6 my-auto">
                  <br>
                  <img src="img/results/variance.png" class="img-responsive" alt="" style="width:100%">
                  <br>
                  <caption><font size="2">Figure 3: Sample PR curves of 4 runs of the same experiment. </font></caption>
                  <br>
                  <br>
                </div>
                <div class = "col-md-6">
                  <h3>Reducing Variance</h3>
                  <p class="text-muted" style="text-align: justify;">
                  Due to variability in the object detection model’s training process, there will be variations between the results of each run, as shown on the left image. Each experiment is therefore repeated 4 times to account for this randomness and improve the accuracy of the result. The average AP value is calculated and used to compare results of our baseline model and model with added synthetic images.</p>
                </div>
          <br>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results</h2>
          <div class="row">
              <div class="col-md-12 text-center">
                <p class="text-muted" style="text-align: justify;">
                  The performances of the model with added synthetic images <b>improve significantly </b> in both within-domain and cross-domain settings.
                  Synthetic images are especially helpful in cross-domain settings, which means they can be useful when there is a lack of data or when it
                  is cost-prohibitive to collect data of the target domain.
                </p>
              </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results.png" class="img-responsive" alt="" style="width:80%">
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results-table.png" class="img-responsive" alt="" style="width:70%">
              <br>
              <caption><font size="2">Figure 4: All values are in average precision (AP). </font></caption>
            </div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-6">
              <img src="img/results/NE-EM-baseline.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 5: Sample test image from the baseline experiment of training on Northeast and testing on Eastern Midwest. The model falsely predicted an architecture to the right as 3 wind turbines. </font></caption>
            </div>
            <div class="col-md-6">
              <img src="img/results/NE-EM-adding-synthetic.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 6: Sample test image from the adding synthetic experiment of training on Northeast and testing on Eastern Midwest. The adding synthetic prediction is able to achieve a higher precision. </font></caption>
              <br>
              <br>
            </div>
            <div class="col-md-1"></div>
          </div>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results of Each Geographic Domain Respectively</h2>
          <div class="row">
            <div class="col-md-12 text-center">
              <p class="text-muted" style="text-align: justify;">
                Here we will present a closer look into the results of training with real images from each of the 3 geographic regions respectively.
                There is a disparity in performance when the model is trained with real images of different geographic domains. In particular, in
                cross-domain experiments that test on Eastern Midwest, the model performs generally worse than when testing on other regions.
              </p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
              <img src="img/results/results-NE.png" class="img-responsive" alt="" style="width:100%">
              <img src="img/results/results-NE-table.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">Figure 7: Results of training with real images from Northeast. </font></caption>
              <br>
              <br>
              <br>
              <br>
            </div>
            <div class="col-md-6 my-auto">
                <img src="img/results/results-NW.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-NW-table.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 8: Results of training with real images from Northwest. </font></caption>
                <br>
                <br>
                <br>
                <br>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
                <img src="img/results/results-EM.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-EM-table.png" class="img-responsive" alt="" style="width:100%">
                <caption><font size="2">Figure 9: Results of training with real images from Eastern Midwest</font></caption>
            </div>
            <div class="col-md-6">
                <p class="text-muted" style="text-align: justify;">
                  As shown on the left and above, the model performs consistently worse when testing on Eastern Midwest, yet Eastern Midwest is also the region where the model
                  has the greatest average improvement in average precision from the addition of synthetic imagery. The exact model performance and improvement synthetic images can make hence depend on the details of specific geographic regions. <br><br> It is therefore possibly more challenging for the model to perform well in certain settings, such as when there are different designs of wind turbines in a region or when the region has more diverse geographic backgrounds. Despite these challenges, synthetic imagery was able to help bridge the gap and bring significant improvement to model performance. </p>
            </div>
          </div>
  </section>


  <!-- Key Takeaways-->
  <section class="page-section" id="key-takeaways">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Key Takeaways</h2>

          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-1 my-auto"></div>
            <div class="col-md-10 my-auto">
              <p class="text-muted" style="text-align: justify;">The results show that adding the curated synthetic imagery improves the performance of
              our object detection model in all cases. This is especially the case when we have a cross domain setting (testing on an unseen region).
              The performance increase is more limited in the within domain setting, where there the model is testing on a previously seen region. We
              also explored the various ratios of real to synthetic imagery and found an optimal ratio. This method of synthetic generation is cheap
              and fast to produce, allowing us to help an object detection model perform on new domains or when we lack training data, which is often the
              case when we are trying to obtain information on energy infrastructure. With the aid of synthetic imagery, this method of collecting
              locations of energy infrastructure could fill in the information gaps that energy access planners need when making decisions about electrification.</p>
            </div>
            <div class="col-md-1 my-auto"></div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-12">
              <img src="img/key_takeaways/Overall Results.png" class="img-responsive" alt="" style="width:50%">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Future Work -->
  <section class="page-section" id="future-work">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Future Work</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <ol class="text-muted" style="text-align: justify;">
                <li>
                  Apply these techniques to <strong>detect other types of energy infrastructure</strong>. Because high voltage transmission towers are
                  similar structure to wind turbines, we can easily adapt our synthetic image generation process for transmission towers. We could then
                  test this synthetic imagery in the same manner as what we performed for the synthetic imagery of wind turbines, and see if this method
                  extends to this other energy infrastructure.
                </li>
                <br>
                  <div class="figure-img">
                    <img src="img/future_work/High_voltage_transmission_towers_and_lines.jpg" class="img-responsive" alt="" title="Img Source: https://upload.wikimedia.org/wikipedia/commons/b/b4/High_voltage_transmission_towers_and_lines.jpg" style="width:50%">
                  </div>
                <br>
                <li>
                  Try few shot learning, where we use <strong>small amounts of real images and large amounts of synthetic data</strong> to adapt our object detection
                  model to any region that we choose.
                </li>
                <br>
                <div class="figure-img">
                    <img src="img/future_work/Few Shot Learning.png" class="img-responsive" alt="" title="Source: https://deepai.org/publication/a-new-benchmark-for-evaluation-of-cross-domain-few-shot-learning" style="width:100%">
                </div>
                <br>
                <li>
                  We would also like to explore <strong>more methods of generating synthetic data</strong>. One method in particular would be using
                  generative adversarial networks (GANs) to generate synthetic imagery with deep learning. This could look more realistic
                  than the current synthetic imagery, since the synthetic image generation process would learn from the real imagery we have.
                </li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Acknowledgements -->
  <section class="bg-light page-section" id="acknowledgements">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Acknowledgements</h2>
        </div>
      </div>
      <br>
      <div class="row justify-content-center">
        <p class="text-muted" style="text-align: justify;"> We would like to thank Dr. Kyle Bradbury, Dr. Jordan Malof, and Wayne Hu for their
          help and guidance along the way. We would also like to thank the previous Bass Connections and Data+ teams for their work leading up
          to this project. We would also like to thank Dr. Rob Fetter, Dr. Marc Jeuland, and Dr. Luana Lima for sharing their work with us.
          Thank you to to the Duke Bass Connections Program and Energy Initiative that supported this project.
        </p>
      </div>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/acknowledgement/Bass-Connections-Logo.jpeg" class="img-responsive" alt="" style="width:100%">
        </div>

        <div class="col-md-6 my-auto">
          <img src="img/acknowledgement/Duke Energy Initiative Logo.png" class="img-responsive" alt="" style="width:100%">
        </div>
       </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="row align-items-center">
        <div class="col-md-6">
          <span class="copyright"></span>
        </div>
        <div class="col-md-6">
          <ul class="list-inline social-buttons">
            <li class="list-inline-item">
              <a href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">
                <i class="fab fa-github"></i>
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Contact form JavaScript -->
  <script src="js/jqBootstrapValidation.js"></script>
  <script src="js/contact_me.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/agency.min.js"></script>

</body>

</html>
