<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Locating Energy Infrastructure with Deep Learning </title>

  <!-- Bootstrap core CSS -->
  <link rel="shortcut icon" type="image/png" href="img/icon.png" />

  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/agency.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav text-uppercase my-auto text-center">
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#methodology">Methodology</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#results">Results</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#key-takeaways">Key Takeaways</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#future-work">Future Work</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="masthead">
    <div class="container">
      <div class="intro-text">
        <div class="intro-lead-in">Bass Connections 2020-2021</div>
        <div class="intro-heading text-uppercase">Deep Learning for Rare Energy Infrastructure in Satellite Imagery
        </div>
        <p class="text-white-75 font-weight-light mb-5" style="font-size: 30px">Names go here</p>
        <a class="btn btn-primary btn-xl js-scroll-trigger" href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">See Our
          Github Repository</a>
        <a class="btn btn-primary btn-alt js-scroll-trigger" href="https://figshare.com/articles/dataset/Baseline_Dataset_and_Synthetic_Images/13626377"> Dataset </a>
      </div>
    </div>
  </header>


  <!-- Motivation -->
  <section class="page-section" id="motivation">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Motivation</h2>

          <hr class="divider light my-4" />
        </div>
      </div>
      <div class="row justify-content-center">
        <div class="col-md-12">
          <h3>Energy Access Planning</h3>
          <p class="text-muted">Access to electricity is important for promoting economic development along with improving living conditions.
            Around <a href="https://energyeducation.ca/encyclopedia/Access_to_electricity">1.2 billion people worldwide do not have electricity in their homes</a>, 
            many of them located in Africa and Asia. The first step in this process is to understand the location of existing energy infrastructure, which helps enable analysis of 
            the distribution of energy resources as well as the consumption of energy. Doing this through surveys
            is incredibly time-consuming, which is why we are trying to locate energy infrastructure automatically by applying deep learning techniques to overhead imagery.
          </p>
        </div>
      </div>

      <hr class="divider light my-4" />

      <div class="row text-center">
        <div class="col-md-12">
          <h3 style="text-align: left;">Deep Learning and Object Detection</h3>
          <p class="text-muted" style="text-align: justify;">Using deep learning, we can feed an image to a model, and the model is able to make predictions about the contents or characteristics of that image. A common
            technique for image analysis is classification, in which the model predicts the class of the image out of a list of possible classes.
            In the image below, the model predicts that the image is of a cat, and is 90% confident with this classification. The model learns how to predict these classifications based
            on examples that are shown to it. After it has been trained, it can classify images that it has not seen before.<br>
            <br>
            For this project, we are focusing on object detection, which is a combination of classification with localization. The model analyzes images and predicts bounding boxes that
            surround each object. It then also classifies each object, producing a confidence score corresponding to the prediction. In the image below, the model predicted that there was
            an object in the box shown in red, and also predicted that the object within that box is a cat. The model learns how to predict these boxes and classifications based on examples shown to it.
            These examples have labels that we refer to as ground truth that contain the information of where the objects are in the image.
          </p>
        </div>
        <div class="col-md-12">
          <img src="img/overview/deep_learning.png" class="img-responsive" alt=""style="width:100%">
          <caption><i>Images from: <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">
            http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf</a></i></caption>
        </div>
      </div>

      <hr class="divider light my-4" />
			
      <div class="row text-center">
        <div class="col-md-6">
          <h3 class="service-heading">Background</h3>
          <p class="text-muted" style="text-align: justify;"> 
            For five years, the Duke Energy Data Analytics Lab has worked on developing deep learning models that identify energy infrastructure, with an end goal of
            generating maps of power grid networks that can aid policymakers in implementing effective
            electrification strategies. In 2015-16, researchers created a model that can detect solar photovoltaic arrays with high accuracy <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-2015-2016">[2015-16
                Bass Connections Team]</a>.
            In 2018-19, this model was improved to identify different types of transmission and distribution energy infrastructures,
            including power lines and transmission towers <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-energy-infrastructure-map-world-through-satellite-data-2018">[2018-19
                Bass Connections Team]</a>. Last year's project focused on increasing the adaptability of detection models
            across different geographies by creating realistic synthetic imagery <a
                href="https://bass-connections-2019.github.io/">[2019-20
            Bass Connections Team]</a>. In our project, we build upon this progress and try to improve the model's ability to detect rare objects.
          </p>
        </div>
        <div class="col-md-6">
          <img src="img/overview/energy_information.png" class="img-responsive center" alt=""
          style="width:100%">
        </div>
      </div>
			  
			<hr class="divider light my-4" />

			<div class="row text-center">
			  <div class="col-md-6">
          <div class="col-md-12 my-auto"><img src="img/overview/wind.jpg" class="img-responsive" alt=""
          style="width:80%"></div>
        </div>
			  <div class="col-md-6">
          <h3 class="service-heading">Challenge: Rare Objects</h3>
          <p class="text-muted" style="text-align: justify;"> Object detection networks, like the one used in this work, are notorious for their "data hunger," requiring 
            <strong>large amounts of annotated training data to perform well</strong>. 
            For common infrastructure like buildings and roads, there is ample real-world data available to train such models.
            However, for rare objects like wind turbines, there is not enough available imagery to satisfy the data requirements of these
            models. Further, due to their rarity and low spatial density in overhead imagery, acquiring more data can be very expensive.
          </p>
        </div>
      </div>
      <br>

      <hr class="divider light my-4" />
      
      <div class="row">
        <div class="col-md-12">
          <h3>Solution: Synthetic Imagery</h3>
          <p class="text-muted" style="text-align: justify;">
            Since training data is difficult to collect, in this project we explore creating synthetic data to supplement the real data that are available.
            We do this using CityEngine, which can render and generate 3D landscapes and structures
            based on input from the designer. In our case, we are populating a landscape with wind turbine models.
            Then we can position the camera in the overhead position and capture images with similar appearances to overhead imagery. 
            Since we placed the wind turbines in the synthetic image, we can also generate ground truth labels for each of these images.
          </p>
        </div>
        <div class="col-md-12">
          <img src="img/creating_synthetic_data/cityengine_workflow.png" class="img-responsive" alt=""style="width:100%">
        </div>
      </div>
			  	  
      <hr class="divider light my-4" />

      <div class="row">
        <div class="col-md-12 my-auto">
          <h3 class="service-heading">Methods</h3>
          <p class="text-muted" style="text-align: justify;">
            Here you can see our roadmap for the research project. In the following sections, we will discuss each
            of these steps.
          </p>
        </div>

        <div class="col-md-12 my-auto">
          <img src="img/overview/research_steps_colored.png" class="img-responsive" alt=""style="width:100%">
        </div>
      </div>

		</div>
	</section>
  
  <!-- Methodology -->
  <section class="page-section" id="methodology">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Methodology</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                Below you can see an outline of the steps required to run these experiments. We'll take about each of them in this section.
              </p>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Overview of Steps.png" class="img-responsive" alt=""style="width:100%">
            </div>
          </div>
          <hr class="divider light my-4" />
          <h2 class="service-heading text-left">Collecting Real Imagery</h2>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> For our overhead imagery of wind turbines, we chose to sample them from 
                <a href="https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/">NAIP</a>.
                This imagery covers a large part of the US and is very high resolution, making it great for our experiments. 
                We collected imagery in three different regions that we called Northwest, Northeast, and Eastern Midwest.
              </p>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northwest</h5>
              <dl>
                <dd class="text-muted">- Overall hue is brown</dd>
                <dd class="text-muted">- Mostly desert and grassland</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northeast</h5>
              <dl>
                <dd class="text-muted">- Hue is very green</dd>
                <dd class="text-muted">- Mostly forests</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Eastern Midwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly green, some brown</dd>
                <dd class="text-muted">- Primarily farmland</dd>
              </dl>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Collecting Overhead Imagery.png" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: Each dot represents a single image that we collected.</i></caption>
            </div>
          </div>
          <br>
          <br>
          <div class="row">
            <p class="text-muted" style="text-align: justify;"> Below we can see the regions splits by which states they include,
              as well as the number of images we collected for each region.
            </p>
            <img src="img/methodology/Where Our Data is From.png" class="img-responsive" alt="" style="width:90%">
          </div>
        </div>
      </div>

      
      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Creating Synthetic Imagery</h2>
          <p class="text-muted" style="text-align: left;"> To create synthetic imagery, we use a software called CityEngine. As inputs to 
            this process, we need a list of background images that do not contain any wind turbines and a 3D model of a wind turbine. We then can automatically generate synthetic images 
            as seen on the right. First, a randomly chosen background image is placed, and then the turbine model is generated randomly
            on top of the background image to create the 3D scene shown in the middle of the figure below. Next, a simulated camera is moved 
            to the overhead / bird's eye view, and then we can save and export this overhead synthetic image.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Synthetic Generation Process.png" class="img-responsive" alt="" style="width:100%">
        </div>
      </div>

      <br><br>
      <div class="row">
        <div class="col-md-4 my-auto">
          <p class="text-muted" style="text-align: justify;"> We can repeat this process but remove the background images, and color
            in the turbine models as black to retrieve information on where the turbine models are located. These black-and-white images can
            then be used to generate formatted labels that we can use with our object detection model.
          </p>
        </div>
        <div class="col-md-8 my-auto">
          <img src="img/methodology/Generating Annotations.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Figure: Side-by-side of an RGB image with its corresponding black-and-white label.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: justify;"> These synthetic images are simple, cheap, and fast to create. All we need are
            background images, and the rest of the process of generating the images and the labels is done automatically, making this a great
            alternative when we don't have enough real imagery or when it is time-consuming or expensive to collect. Below are some example
            synthetic images with a variety of background images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Example Synthetic Images.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Example synthetic images.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <h3 class="service-heading">Synthetic Imagery Design Considerations</h3>
          <p class="text-muted" style="text-align: left;"> The design of the synthetic imagery is important because the closer the
            synthetic imagery is to the real imagery, the more the synthetic imagery will improve our performance when adding it to our 
            training set.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Turbine Size Distribution.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Size of the Synthetic Turbines</h5>
          <p class="text-muted" style="text-align: left;"> The first consideration we have to make is what size to make the
            synthetic turbines. For this issue, we chose to model the size distribution of the synthetic turbines after the size
            distribution of the real turbines. To do this, we created a histogram of the size of the turbines in our real imagery (see left),
            and modeled this in our synthetic imagery with multiple bins of uniform distributions.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Angle of the Camera</h5>
          <p class="text-muted" style="text-align: left;"> The next decision we had to make about the synthetic imagery design was
            the angle of the simulated camera when we are capturing photos of the synthetic 3D scene. We noticed that in the real imagery,
            some of the images were captured at an angle. This can be seen in the real images to the right, where we can see the pole of 
            the turbine due to the angle of the camera. We tried to incorporate this into our synthetic imagery, where we would pick
            to either take the image from directly above or to have a randomly chosen camera angle between 60 and 90 degrees.
          </p>
        </div>
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Angle of Camera 2.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
      </div>
      <div class="row">
        <h3 class="service-heading">Testing the Synthetic Imagery</h3>
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: left;"> Now that we have some synthetic imagery, we want to test whether it can
            help the performance of our object detection model when we deploying it on a new geography or when we lack training data. 
            To test this, we can train our model on two different training datasets. One contains purely real imagery, while the second
            contains the same real imagery, but we add in a number of synthetic images. We then test the performance of both models and
            compare the performances.
          </p>

        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Testing Synthetic Imagery.png" class="img-responsive" alt="" style="width:80%"> <br><br>
          <p class="text-muted" style="text-align: left;"> Next, we will go into the process of constructing these two datasets and then discuss the 
            variety of experiments that we completed with the synthetic imagery.
          </p>
        </div>
      </div>
      <hr class="divider light my-4" />

      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Constructing the Datasets</h2>
        </div>
      </div>

    </div>
  </section>

  <!-- Results -->
  <section class="page-section" id="results">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Results</h2>

          <hr class="divider light my-4" />

          <div class="row">

            <div class="col-md-12 my-auto">
              <h2 style="text-align: left;">Basic Experimental Setup</h2>
              <p class="text-muted" style="text-align: justify;">
                Once we have our synthetic data created and our real imagery preprocessed, we are ready to run experiments. 
                These experiments involve altering the composition of the training dataset to see how the performance of the model changes.
                The basic experimental setup for this project is to train the model on a control condition with just real data, 
                and then to use the same dataset but supplement it with synthetic images, and then measure the difference in performance. This will
                produce two different models and their performance is measured on the same testing set. With this setup, we are only changing
                one variable and so we can attribute any differences in performance to the added synthetic data.

              </p>
            </div>

            <div class="col-md-1 my-auto"></div>
            <div class="col-md-10 my-auto">
              <img src="img/experimentation_and_results/experimental_setup.png" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: Our basic experimental setup. The real images were split randomly between the training and testing datasets. 
                The real images are a mix of those with wind turbines present and those without.
              </i></caption>
            </div>
            <div class="col-md-1 my-auto"></div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-6 my-auto">
              <br>
              <img src="img/experimentation_and_results/metrics.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><i>Figure: The model predicted that 4 objects were wind turbines,
                and 2 of those predictions were correct, meaning the precision would be 2/4 for this example. There are 3 wind turbines in the image
                and the model found 2 of these, meaning the recall would be 2/3.</i></caption>
            </div>
            <div class = "col-md-6">
              <h3>Performance Metrics</h3>
              <p class="text-muted" style="text-align: justify;">To understand our results, it's important
              that we first understand the metrics that we have chosen to measure performance. The primary metrics we look at are
              precision and recall. We'll explain how these metrics can be interpreted in terms of the images on the left.</p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Precision: </b> Out of the objects that the model classified as a wind turbine,
                what fraction of these were actually wind turbines.</li>
                <li class="text-muted" style="text-align: justify;"><b>Recall: </b> Of the wind turbines present in the data, what fraction of these 
                did the model find.</li>
              </ul>
              <p class="text-muted" style="text-align: justify;">Since the goal of this model is to detect objects in very large sets of overhead imagery, 
                manually removing objects misclassified as wind turbines is much easier than identifying undetected wind turbines across the entire dataset. 
                For this reason, we prioritize recall over precision as a metric of good performance.
              </p>
            </div>   
          </div>
          <br>
          <div class="row">
            <div class="col-md-6">
              <h3>Results</h3>
              <p class="text-muted" style="text-align: justify;">Adding synthetic data to our training set significantly improved the precision of our
                model. However, the recall does not change significantly. Our hypothesis for this is that with the added synthetic data, model was doing well on large wind turbines
                while missing many of the small wind turbines. This prompted us to create synthetic data that contained small wind turbines to try to 
                fix this issue.
              </p>
            </div>
            <div class="col-md-6">
              <img src="img/experimentation_and_results/results.png" class="img-responsive" alt="" style="width:100%">
            </div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-4">
              <img src="img/experimentation_and_results/large_turbines.png" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: The model performs well on large wind turbines</i></caption>
            </div>
            <div class="col-md-2"></div>
            <div class="col-md-4">
              <img src="img/experimentation_and_results/small_turbines.png" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: The model is inconsistent on small wind turbines</i></caption>
            </div>
            <div class="col-md-1"></div>
          </div>
          <br>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Additional Experiments</h2>
          <div class="row">
            <div class="col-md-12 text-center">
              <p class="text-muted" style="text-align: justify;">In addition to this first experiment, we also conducted additional experiments by changing
              how the dataset was split between the training and testing. One experiment that we explored was excluding a geographic region from the testing set.
              We wanted to choose a region that would be the most difficult for the model to perform on without having training data to learn from. We chose to 
              exclude California and Arizona from the testing set because the geography is both self-similar and fairly different
              from the other regions and also because the small wind turbines in California would be very difficult for the model to test on. In this case, our training set contains
              no small wind turbines while the testing set does contain small turbines.
              </p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-12 my-auto">
              <img src="img/experimentation_and_results/excluding_CA_AZ.png" class="img-responsive" alt="">
              <hr class="divider light my-4"/>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;">After testing the model with this setup, we then were able to add synthetic data to see if the performance
              improves. We specifically created synthetic data that mimicked the real overhead imagery in California and Arizona. In order to do this, we made a histogram of the sizes of the wind turbines located in CA and AZ
              and tried to match that size distribution in our synthetic data. We then added these to the training dataset
              and observed how the performance changed.</p>
            </div>
            <div class="col-md-6 my-auto">
              <figure>
                <img src="img/experimentation_and_results/syn_CA_AZ.png" class="img-fluid">
                <figcaption><i>Figure: Examples of synthetic data created to mimic CA and AZ</i></figcaption>
              </figure>
            </div>
          </div>
          <div class="row">
            <div class="col-md-12 my-auto">
              <figure>
                <img src="img/experimentation_and_results/turbine_sizes_real_CA_AZ.png" class="img-fluid">
                <figcaption><i>Figure: Distribution of wind turbine sizes in CA and AZ by pixel</i></figcaption>
              </figure>
            </div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-6 my-auto">
              <img src="img/experimentation_and_results/results_excluding_CA_AZ.png" class="img-fluid">
            </div>
            <div class="col-md-6 my-auto">
              <h2>Results</h2>
              <p class="text-muted" style="text-align: justify;">After adding synthetic data, the precision decreases a significant amount while the recall improves very slightly.
              This indicates that we need to improve the quality of our synthetic data to better replicate the real world overhead images from California and Arizona. This could include
              purchasing a more realistic model for our small wind turbines or increasing the variety of synthetic data by randomizing lighting conditions.</p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-4">
              <img src="img/experimentation_and_results/large_turbines_CA.jpg" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: The model performs fairly well on large wind turbines</i></caption>
            </div>
            <div class="col-md-2"></div>
            <div class="col-md-4">
              <img src="img/experimentation_and_results/small_turbines_CA.jpg" class="img-responsive" alt="" style="width:100%">
              <caption><i>Figure: The model performs badly on small wind turbines. This indicates to us that we need to improve the quality of the small wind turbines in our synthetic data</i></caption>
            </div>
            <div class="col-md-1"></div>
          </div>
        </div>
      </div>
  </section>


  <!-- Key Takeaways-->
  <section class="page-section" id="key-takeaways">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Key Takeaways</h2>

          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-1 my-auto"></div>
            <div class="col-md-10 my-auto">
              <p style="text-align: justify;">The goal of this project was to create synthetic data that could be added to our training set to improve performance.
              This goal was met, but we think that with the fine-tuning of our synthetic data, we should be able to increase the performance more, especially in
              terms of the recall. For our additional experiment of testing on novel geographies and turbines, the results indicate that the synthetic data we made to mimic the real overhead imagery
              in California and Arizona needs to be improved in terms of its quality, especially the model for the small wind turbine.</p>
            </div>
            <div class="col-md-1 my-auto"></div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Future Work -->
  <section class="page-section" id="future-work">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Future Work</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <ol style="text-align: justify;">
                <li>Develop a <strong>larger variety of synthetic images</strong> for training. This includes varying the lighting of the CityEngine scenes along with including different variations of models.
                  We would like to include a wire structured wind turbine in the synthetic data to improve performance on those types of turbines
                </li><br>
                <li>Improve the <strong>quality of our synthetic images</strong>. This would involve purchasing high-quality models from online or designing models ourselves to emulate the 
                  real wind turbines that we observe in the overhead imagery. This is especially important for the smaller turbines, since performace improved very slightly after adding our synthetic data that included small turbines.
                </a></li><br>
                <li><strong>Apply this model on a large scale</strong>. We would train the model and then apply it to Google Earth Engine imagery for the United States to locate wind turbines throughout the U.S.</li><br>
                <li>Apply these techniques to <strong>detect other types of energy infrastructure</strong>. This will likely include coal plants and transmission lines</li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Team -->
  <section class="bg-light page-section" id="team">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Team</h2>
        </div>
      </div>
      <br>
      <div class="row justify-content-center">   
        <div class="col-md-4">
          <h4 class="service-heading"><u>Team Leaders:</u></h4>
          <p class="text-muted">Dr. Kyle Bradbury</p>
          <p class="text-muted">Dr. Jordan Malof</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="row align-items-center">
        <div class="col-md-6">
          <span class="copyright"></span>
        </div>
        <div class="col-md-6">
          <ul class="list-inline social-buttons">
            <li class="list-inline-item">
              <a href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">
                <i class="fab fa-github"></i>
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Contact form JavaScript -->
  <script src="js/jqBootstrapValidation.js"></script>
  <script src="js/contact_me.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/agency.min.js"></script>

</body>

</html>