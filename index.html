<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Locating Energy Infrastructure with Deep Learning </title>

  <!-- Bootstrap core CSS -->
  <link rel="shortcut icon" type="image/png" href="img/icon.png" />

  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/agency.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav text-uppercase my-auto text-center">
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#methodology">Methodology</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#results">Results</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#key-takeaways">Key Takeaways</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#future-work">Future Work</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="masthead">
    <div class="container">
      <div class="intro-text">
        <div class="intro-lead-in">Bass Connections 2020-2021</div>
        <div class="intro-heading text-uppercase">Deep Learning for Rare Energy Infrastructure in Satellite Imagery
        </div>
        <p class="text-white-75 font-weight-light mb-5" style="font-size: 30px">Names go here</p>
        <a class="btn btn-primary btn-xl js-scroll-trigger" href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">See Our
          Github Repository</a>
        <a class="btn btn-primary btn-alt js-scroll-trigger" href="https://figshare.com/articles/dataset/Baseline_Dataset_and_Synthetic_Images/13626377"> Dataset </a>
      </div>
    </div>
  </header>


  <!-- Motivation -->
  <section class="page-section" id="motivation">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Motivation</h2>

          <hr class="divider light my-4" />
        </div>
      </div>
      <div class="row justify-content-center">
        <div class="col-md-12">
          <h3>Energy Access Planning</h3>
          <p class="text-muted">Access to electricity is important for promoting economic development along with improving living conditions.
            Around <a href="https://energyeducation.ca/encyclopedia/Access_to_electricity">1.2 billion people worldwide do not have electricity in their homes</a>, 
            many of them located in Africa and Asia. The first step in this process is to understand the location of existing energy infrastructure, which helps enable analysis of 
            the distribution of energy resources as well as the consumption of energy. Doing this through surveys
            is incredibly time-consuming, which is why we are trying to locate energy infrastructure automatically by applying deep learning techniques to overhead imagery.
          </p>
        </div>
      </div>

      <hr class="divider light my-4" />

      <div class="row text-center">
        <div class="col-md-12">
          <h3 style="text-align: left;">Deep Learning and Object Detection</h3>
          <p class="text-muted" style="text-align: justify;">Using deep learning, we can feed an image to a model, and the model is able to make predictions about the contents or characteristics of that image. A common
            technique for image analysis is classification, in which the model predicts the class of the image out of a list of possible classes.
            In the image below, the model predicts that the image is of a cat, and is 90% confident with this classification. The model learns how to predict these classifications based
            on examples that are shown to it. After it has been trained, it can classify images that it has not seen before.<br>
            <br>
            For this project, we are focusing on object detection, which is a combination of classification with localization. The model analyzes images and predicts bounding boxes that
            surround each object. It then also classifies each object, producing a confidence score corresponding to the prediction. In the image below, the model predicted that there was
            an object in the box shown in red, and also predicted that the object within that box is a cat. The model learns how to predict these boxes and classifications based on examples shown to it.
            These examples have labels that we refer to as ground truth that contain the information of where the objects are in the image.
          </p>
        </div>
        <div class="col-md-12">
          <img src="img/overview_OLD/deep_learning.png" class="img-responsive" alt=""style="width:100%">
          <caption><i>Images from: <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">
            http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf</a></i></caption>
        </div>
      </div>

      <hr class="divider light my-4" />
			
      <div class="row text-center">
        <div class="col-md-6">
          <h3 class="service-heading">Background</h3>
          <p class="text-muted" style="text-align: justify;"> 
            For five years, the Duke Energy Data Analytics Lab has worked on developing deep learning models that identify energy infrastructure, with an end goal of
            generating maps of power grid networks that can aid policymakers in implementing effective
            electrification strategies. In 2015-16, researchers created a model that can detect solar photovoltaic arrays with high accuracy <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-2015-2016">[2015-16
                Bass Connections Team]</a>.
            In 2018-19, this model was improved to identify different types of transmission and distribution energy infrastructures,
            including power lines and transmission towers <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-energy-infrastructure-map-world-through-satellite-data-2018">[2018-19
                Bass Connections Team]</a>. Last year's project focused on increasing the adaptability of detection models
            across different geographies by creating realistic synthetic imagery <a
                href="https://bass-connections-2019.github.io/">[2019-20
            Bass Connections Team]</a>. In our project, we build upon this progress and try to improve the model's ability to detect rare objects.
          </p>
        </div>
        <div class="col-md-6">
          <img src="img/overview_OLD/energy_information.png" class="img-responsive center" alt=""
          style="width:100%">
        </div>
      </div>
			  
			<hr class="divider light my-4" />

			<div class="row text-center">
			  <div class="col-md-6">
          <div class="col-md-12 my-auto"><img src="img/overview_OLD/wind.jpg" class="img-responsive" alt=""
          style="width:80%"></div>
        </div>
			  <div class="col-md-6">
          <h3 class="service-heading">Challenge: Rare Objects</h3>
          <p class="text-muted" style="text-align: justify;"> Object detection networks, like the one used in this work, are notorious for their "data hunger," requiring 
            <strong>large amounts of annotated training data to perform well</strong>. 
            For common infrastructure like buildings and roads, there is ample real-world data available to train such models.
            However, for rare objects like wind turbines, there is not enough available imagery to satisfy the data requirements of these
            models. Further, due to their rarity and low spatial density in overhead imagery, acquiring more data can be very expensive.
          </p>
        </div>
      </div>
      <br>

      <hr class="divider light my-4" />
      
      <div class="row">
        <div class="col-md-12">
          <h3>Solution: Synthetic Imagery</h3>
          <p class="text-muted" style="text-align: justify;">
            Since training data is difficult to collect, in this project we explore creating synthetic data to supplement the real data that are available.
            We do this using CityEngine, which can render and generate 3D landscapes and structures
            based on input from the designer. In our case, we are populating a landscape with wind turbine models.
            Then we can position the camera in the overhead position and capture images with similar appearances to overhead imagery. 
            Since we placed the wind turbines in the synthetic image, we can also generate ground truth labels for each of these images.
          </p>
        </div>
        <div class="col-md-12">
          <img src="img/creating_synthetic_data_OLD/cityengine_workflow.png" class="img-responsive" alt=""style="width:100%">
        </div>
      </div>
			  	  
      <hr class="divider light my-4" />

		</div>
	</section>
  
  <!-- Methodology -->
  <section class="page-section" id="methodology">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Methodology</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                Below you can see an outline of the steps required to run these experiments. We'll take about each of them in this section.
              </p>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Overview of Steps.png" class="img-responsive" alt=""style="width:100%">
            </div>
          </div>
          <hr class="divider light my-4" />
          <h2 class="service-heading text-left">Collecting Real Imagery</h2>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> For our overhead imagery of wind turbines, we chose to sample them from 
                <a href="https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/">NAIP</a>.
                This imagery covers a large part of the US and is very high resolution, making it great for our experiments. 
                We collected imagery in three different regions that we called Northwest, Northeast, and Eastern Midwest.
              </p>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northwest</h5>
              <dl>
                <dd class="text-muted">- Overall hue is brown</dd>
                <dd class="text-muted">- Mostly desert and grassland</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northeast</h5>
              <dl>
                <dd class="text-muted">- Hue is very green</dd>
                <dd class="text-muted">- Mostly forests</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Eastern Midwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly green, some brown</dd>
                <dd class="text-muted">- Primarily farmland</dd>
              </dl>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Collecting Overhead Imagery.png" class="img-responsive" alt="" style="width:80%"><br>
              <caption><i>Figure: Each dot represents a single image that we collected.</i></caption>
            </div>
          </div>
          <br>
          <br>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> Below we can see the regions splits by which states they include,
                as well as the number of images we collected for each region.
              </p>
              <img src="img/methodology/Where Our Data is From.png" class="img-responsive" alt="" style="width:70%">
            </div>
          </div>
        </div>
      </div>

      
      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Creating Synthetic Imagery</h2>
          <p class="text-muted" style="text-align: left;"> To create synthetic imagery, we use a software called CityEngine. As inputs to 
            this process, we need a list of background images that do not contain any wind turbines and a 3D model of a wind turbine. We then can automatically generate synthetic images 
            as seen on the right. First, a randomly chosen background image is placed, and then the turbine model is generated randomly
            on top of the background image to create the 3D scene shown in the middle of the figure below. Next, a simulated camera is moved 
            to the overhead / bird's eye view, and then we can save and export this overhead synthetic image.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Synthetic Generation Process.png" class="img-responsive" alt="" style="width:100%">
        </div>
      </div>

      <br><br>
      <div class="row">
        <div class="col-md-4 my-auto">
          <p class="text-muted" style="text-align: justify;"> We can repeat this process but remove the background images, and color
            in the turbine models as black to retrieve information on where the turbine models are located. These black-and-white images can
            then be used to generate formatted labels that we can use with our object detection model.
          </p>
        </div>
        <div class="col-md-8 my-auto">
          <img src="img/methodology/Generating Annotations.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Figure: Side-by-side of an RGB image with its corresponding black-and-white label.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: justify;"> These synthetic images are simple, cheap, and fast to create. All we need are
            background images, and the rest of the process of generating the images and the labels is done automatically, making this a great
            alternative when we don't have enough real imagery or when it is time-consuming or expensive to collect. Below are some example
            synthetic images with a variety of background images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Example Synthetic Images.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Example synthetic images.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <h3 class="service-heading">Synthetic Imagery Design Considerations</h3>
          <p class="text-muted" style="text-align: left;"> The design of the synthetic imagery is important because the closer the
            synthetic imagery is to the real imagery, the more the synthetic imagery will improve our performance when adding it to our 
            training set.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Turbine Size Distribution.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Size of the Synthetic Turbines</h5>
          <p class="text-muted" style="text-align: left;"> The first consideration we have to make is what size to make the
            synthetic turbines. For this issue, we chose to model the size distribution of the synthetic turbines after the size
            distribution of the real turbines. To do this, we created a histogram of the size of the turbines in our real imagery (see left),
            and modeled this in our synthetic imagery with multiple bins of uniform distributions.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Angle of the Camera</h5>
          <p class="text-muted" style="text-align: left;"> The next decision we had to make about the synthetic imagery design was
            the angle of the simulated camera when we are capturing photos of the synthetic 3D scene. We noticed that in the real imagery,
            some of the images were captured at an angle. This can be seen in the real images to the right, where we can see the pole of 
            the turbine due to the angle of the camera. We tried to incorporate this into our synthetic imagery, where we would pick
            to either take the image from directly above or to have a randomly chosen camera angle between 60 and 90 degrees.
          </p>
        </div>
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Angle of Camera 2.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Nearby Background Image.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Test image and nearby collected background image.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Which Background Images to Use</h5>
          <p class="text-muted" style="text-align: left;"> We also have to choose which background images to have placed under our
            synthetic data. We chose to collect background imagery close to the real images in our testing set so that our synthetic
            imagery would look as close as possible to the target data. We would not be able to do this if we were deploying the model, 
            since we wouldn't know our testing data, but we would be able to collect imagery from around the region as our background images.
            However, using the background images close to our testing locations for these experiments allows us to search for an upper bound
            on the performance increase that the synthetic data can provide, and can also tell us how important the background images are when 
            we are creating our synthetic images.
          </p>
        </div>
      </div>
      <div class="row">
        <h3 class="service-heading">Testing the Synthetic Imagery</h3>
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: left;"> Now that we have some synthetic imagery, we want to test whether it can
            help the performance of our object detection model when we deploying it on a new geography or when we lack training data. 
            To test this, we can train our model on two different training datasets. One contains purely real imagery, while the second
            contains the same real imagery, but we add in a number of synthetic images. We then test the performance of both models and
            compare the performances.
          </p>

        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Testing Synthetic Imagery.png" class="img-responsive" alt="" style="width:80%"> <br><br>
          <p class="text-muted" style="text-align: left;"> Next, we will go into the process of constructing these two datasets and then discuss the 
            variety of experiments that we completed with the synthetic imagery.
          </p>
        </div>
      </div>
      <hr class="divider light my-4" />

      <div class="row">
        <div class="col-md-12 my-auto">
        <h2 class="service-heading">Constructing the Datasets</h2>
          
        <div class="row">
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Clustering and Stratified Sampling</h5>
              <p class="text-muted" style="text-align: justify;"> Because our self-defined geographic regions are wide and diverse, it's important for our training and testing datasets are representative of a geographic region. To ensure our datasets are representative, we k-means clustering within each region and then stratified sample from each cluster to construct our 100:100 (train to test ratio) constrained baseline datasets to increase homogeneity within a region. </p>
          </div>
          <div class="col-md-6 my-auto">
            <img src="img/methodology/Clustering and Stratified Sampling.png" class="img-responsive" alt="" style="width:100%">
          </div>
        </div>
          
          <h5 class="service-heading">Optimizing Real to Synthetic Ratio</h5>
          <div class="row">
            
            <div class="col-md-6 my-auto">
              <img src="img/methodology/Ratio Test Experimental Design.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
            </div>
            
            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;"> In order to construct our baseline and adding synthetic datasets, we need to figure out what real to synthetic ratio yields the most increase in performance. To find this ratio, we designed the following experiment, where we test 1:0, 1:0.5, 1:0.75, 1:1, and 1:2 real to synthetic ratios. After conducting these experiments, we found that 1:0.75 yields the largest performance boost. Therefore, we design our experiments using the 1:0.75 ratio.  </p>
             </div>
        </div>
      
      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Experimental Setup</h2>
          <br>
          <p class="text-muted" style="text-align: justify;">Having clustered and sampled our data and found the optimal real to synthetic ratio, our final datasets for each region is: </p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Baseline: </b> Train on 100 Real Non-Target Images, Validate on 100 Target Domain Images</li>
                <li class="text-muted" style="text-align: justify;"><b>Modified: </b> Train on 100 Real Non-Target Images + 75 Syn Target Images, Validate on 100 Target Domain Images </li>
              </ul>
          </br>
          
          <h5 class="service-heading">Overview</h5>
          <div class="row">     
            <div class="col-md-6 my-auto">
              <p>Our goal is to test how well synthetic imagery improves performance when the model is testing on a new geography. To achieve this goal, we conduct cross-domain experiments, where we train on the source domain, but we validate on the target domain. To see the effects of synthetic imagery, we add synthetic imagery from the target domain in our modified datasets. We also conducted within domain experiments, where we train and validate on the target domain, to establish baseline performance. In our case, each domain is a geographic region that we defined. Namely, Northwest, Northeast, and Eastern Midwest. </p>
             </div>
            <div class="col-md-6 my-auto">
              <img src="img/methodology/Overall Experimental Setup.png" class="img-responsive" alt="" style="width:100%">
            </div> 
           
           <div class="col-md-6 my-auto">
             <img src="img/methodology/Pairwise Cycle.png" class="img-responsive" alt="Pairwise Experiment Setup" style="width:100%">
            </div>
            
            <div class="col-md-6 my-auto">
             <p> To test the impact of each region, we run a 3-way pairwise experiment, where we train and validate on each of the 3 regions, resulting in 6 experiments. </p>
            </div>
          </div>
          
            
          <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Within Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: Within-domain experiment example using Northwest as target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Within Domain Experiment</h5>
             <p class="text-muted" style="text-align: left;"> Before we test how well synthetic imagery improves performance when the model is testing on a different domain than the one it was trained on, we need to establish some baseline. We need to know how well the model is doing when it's trained and validated on the same domain. Using Northwest region as our target domain, we ran our baseline and modified experiments, and we repeat for the other domains/regions.</p>
          </div>
        </div>
        <br>
        
        
        <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Cross Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: Cross-domain experiment example using Northwest as target domain and Eastern Midwest as non-target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Cross Domain Experiment</h5>
             <p class="text-muted" style="text-align: left;"> We are curious if synthetic data can help overcome differences in geography. To test this, we train on one region, and validate on another region (cross-domain). Then we add synthetic imagery similar to the target domain and compare performance against the baseline. Here we demonstrate this experiment setup using Northwest as the target domain and Eastern Midwest as the non-target domain. We repeat for the other domains as well. </p>
          </div>
        </div>
        <br>
        
       </div>
      </div>

    </div>
  </section>

  <!-- Results-->
  <section class="page-section" id="results">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Results</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-6 my-auto">
              <br>
              <img src="img/results/metrics.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">Figure 1: The model predicted that 4 objects were wind turbines. 2 of those predictions were correct, meaning the precision would be 2/4. There are 3 wind turbines in the image and the model found 2 of these, meaning the recall would be 2/3.</font></caption>
            </div>
            <div class = "col-md-6">
              <h3>Performance Metrics</h3>
              <p class="text-muted" style="text-align: justify;">To understand our results, it's important that we first understand the metrics that we have chosen to measure performance. The primary metrics we will use is Average Precision for an IoU threshold of 0.5 (AP@0.5). We will explain the implication of this metric starting with the images on the left.</p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Precision: </b> Out of the objects that the model classified as a wind turbine,
                what fraction of these were actually wind turbines.</li>
                <li class="text-muted" style="text-align: justify;"><b>Recall: </b> Out of wind turbines present in the data, what fraction of these
                did the model find.</li>
              </ul>
            </div>
            <div class="row">
              <div class = "col-md-6 my-auto">
                <p class="text-muted" style="text-align: justify;">Now we plot the values of precision and recall a model can reach when doing predictions on a graph, which is known as a precision-recall curve. You can see on the curve that as precision increases, recall decreases, and vice versa. There is hence a tradeoff between precision and recall. However, we would like to have high values for both precision and recall, which means we would like the area under the precision-recall curve to be as high as possible. A metric that quantifies this area is Average Precision (AP). </p>
                <p class="text-muted" style="text-align: justify;">Note that in the machine learning space, a small absolute increase in AP is already a significant improvement.
                </p>
              </div>
              <div class="col-md-6">
                <br>
                <img src="img/results/sample_pr.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 2: Sample Precision Recall Curve. We would like the curve to move to right as much as possible as indicated by the green arrow. </font></caption>
                <br>
                <br>
                <br>
              </div>
              <div class="row">
                <div class="col-md-6 my-auto">
                  <br>
                  <img src="img/results/variance.png" class="img-responsive" alt="" style="width:100%">
                  <br>
                  <caption><font size="2">Figure 3: Sample PR curves of 4 runs of the same experiment. </font></caption>
                  <br>
                  <br>
                </div>
                <div class = "col-md-6">
                  <h3>Reducing Variance</h3>
                  <p class="text-muted" style="text-align: justify;">Due to randomness in machine learning, there will be slight variations between the results of each run, as shown on the left image. Each experiment is therefore repeated 4 times to account for this randomness and improve the accuracy of the result. The average AP value is calculated and used to compare results of our baseline model and model with added synthetic images.</p>
                </div>
          <br>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results</h2>
          <div class="row">
              <div class="col-md-12 text-center">
                <p class="text-muted" style="text-align: justify;"> The performances of the model with added synthetic images <b>improve significantly </b> in both within-domain and cross-domain settings. Synthetic images are especially helpful in cross-domain settings, which means they can be useful when there is a lack of data or when it is cost-prohibitive to collect data of the target domain.
                </p>
              </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results.png" class="img-responsive" alt="" style="width:80%">
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results-table.png" class="img-responsive" alt="" style="width:70%">
              <br>
              <caption><font size="2">Figure 4: All values are in AP@0.5. </font></caption>
            </div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-6">
              <img src="img/results/NE-EM-baseline.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 5: Sample test image from the baseline experiment of training on Northeast and validating on Eastern Midwest. The model falsely predicted an architecture to the right as 3 wind turbines. </font></caption>
            </div>
            <div class="col-md-6">
              <img src="img/results/NE-EM-adding-synthetic.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 6: Sample test image from the adding synthetic experiment of training on Northeast and validating on Eastern Midwest. The adding synthetic prediction is able to achieve a higher precision. </font></caption>
              <br>
              <br>
            </div>
            <div class="col-md-1"></div>
          </div>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results of Each Geographic Domain Respectively</h2>
          <div class="row">
            <div class="col-md-12 text-center">
              <p class="text-muted" style="text-align: justify;">Here we will present a closer look into the results of training with real images from each of the 3 geographic regions respectively. There is a disparity in performance when the model is trained with real images of different geographic domains. In particular, in cross-domain experiments that validate on Eastern Midwest, the model perform generally worse than when valiating on other regions. Meanwhile, the model benefits the most from the addition of synthetic imagery if the baseline model does not perform comparatively well, such as in the case of Eastern Midwest.
              </p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
              <img src="img/results/results-NE.png" class="img-responsive" alt="" style="width:100%">
              <img src="img/results/results-NE-table.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">Figure 7: Results of training with real images from Northeast. </font></caption>
              <br>
              <br>
              <br>
              <br>
            </div>
            <div class="col-md-6 my-auto">
                <img src="img/results/results-NW.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-NW-table.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 8: Results of training with real images from Northwest. </font></caption>
                <br>
                <br>
                <br>
                <br>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
                <img src="img/results/results-EM.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-EM-table.png" class="img-responsive" alt="" style="width:100%">
                <caption><i>Figure 9: Results of training with real images from Eastern Midwest</i></caption>
            </div>
            <div class="col-md-6">
                <p class="text-muted" style="text-align: justify;">As shown above, the model performs consistently worse when validating on Eastern Midwest, yet that is when the model has the greatest improvement in AP from the addition of synthetic imagery. </p>
                <br>
                <p class="text-muted" style="text-align: justify;">One explanation for the disparity from performance in Eastern Midwest is the wide diversity of geographic backgrounds presented in the region's dataset. The exact model performance and improvement synthetic images can make depend on geographic regions. Regions with diverse backgrounds may limit the model's performance. </p>
            </div>
          </div>
  </section>


  <!-- Key Takeaways-->
  <section class="page-section" id="key-takeaways">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Key Takeaways</h2>

          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-1 my-auto"></div>
            <div class="col-md-10 my-auto">
              <p style="text-align: justify;">The results show that adding the curated synthetic imagery improves the performance of
              our object detection model in all cases. This is especially the case when we have a cross domain setting (testing on an unseen region).
              The performance increase is more limited in the within domain setting, where there the model is testing on a previously seen region. We
              also explored the various ratios of real to synthetic imagery and found an optimal ratio. This method of synthetic generation is cheap 
              and fast to produce, allowing us to help an object detection model perform on new domains or when we lack training data, which is often the
              case when we are trying to obtain information on energy infrastructure. With the aid of synthetic imagery, this method of collecting 
              locations of energy infrastructure could fill in the information gaps that energy access planners need when making decisions about electrification.</p>
            </div>
            <div class="col-md-1 my-auto"></div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Future Work -->
  <section class="page-section" id="future-work">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Future Work</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <ol style="text-align: justify;">
                <li>Develop a <strong>larger variety of synthetic images</strong> for training. This includes varying the lighting of the CityEngine scenes along with including different variations of models.
                  We would like to include a wire structured wind turbine in the synthetic data to improve performance on those types of turbines
                </li><br>
                <li>Improve the <strong>quality of our synthetic images</strong>. This would involve purchasing high-quality models from online or designing models ourselves to emulate the 
                  real wind turbines that we observe in the overhead imagery. This is especially important for the smaller turbines, since performace improved very slightly after adding our synthetic data that included small turbines.
                </a></li><br>
                <li><strong>Apply this model on a large scale</strong>. We would train the model and then apply it to Google Earth Engine imagery for the United States to locate wind turbines throughout the U.S.</li><br>
                <li>Apply these techniques to <strong>detect other types of energy infrastructure</strong>. This will likely include coal plants and transmission lines</li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Team -->
  <section class="bg-light page-section" id="team">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Team</h2>
        </div>
      </div>
      <br>
      <div class="row justify-content-center">   
        <div class="col-md-4">
          <h4 class="service-heading"><u>Team Leaders:</u></h4>
          <p class="text-muted">Dr. Kyle Bradbury</p>
          <p class="text-muted">Dr. Jordan Malof</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="row align-items-center">
        <div class="col-md-6">
          <span class="copyright"></span>
        </div>
        <div class="col-md-6">
          <ul class="list-inline social-buttons">
            <li class="list-inline-item">
              <a href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">
                <i class="fab fa-github"></i>
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Contact form JavaScript -->
  <script src="js/jqBootstrapValidation.js"></script>
  <script src="js/contact_me.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/agency.min.js"></script>

</body>

</html>
