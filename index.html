<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Locating Energy Infrastructure with Deep Learning </title>

  <!-- Bootstrap core CSS -->
  <link rel="shortcut icon" type="image/png" href="img/icon.png" />

  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="css/agency.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav text-uppercase my-auto text-center">
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#motivation">Motivation</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#methodology">Methodology</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#results">Results</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#key-takeaways">Key Takeaways</a>
          </li>
          <li class="nav-item my-auto">
            <a class="nav-link js-scroll-trigger" href="#future-work">Future Work</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="masthead">
    <div class="container">
      <div class="intro-text">
        <div class="intro-lead-in">Bass Connections 2020-2021</div>
        <div class="intro-heading text-uppercase">Deep Learning for Rare Energy Infrastructure in Satellite Imagery
        </div>
        <p class="text-white-75 font-weight-light mb-5" style="font-size: 30px">Ada Ye, Eddy Lin, Jose Luis Moscoso, Tyler Feldman, Jessie Ou, Wendy Zhang</p>
        <a class="btn btn-primary btn-xl js-scroll-trigger" href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">See Our
          Github Repository</a>
        <a class="btn btn-primary btn-alt js-scroll-trigger" href="https://figshare.com/articles/dataset/Baseline_Dataset_and_Synthetic_Images/13626377"> Dataset </a>
      </div>
    </div>
  </header>


  <!-- Motivation -->
  <section class="page-section" id="motivation">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Motivation</h2>

          <hr class="divider light my-4" />
        </div>
      </div>
      <div class="row justify-content-center">
        <div class="row">
        <div class="col-md-12">
          <h3>Energy Access Planning</h3>
          <p class="text-muted">Access to electricity is important for promoting economic development along with improving living conditions.
            Around <a href="https://energyeducation.ca/encyclopedia/Access_to_electricity">1.2 billion people worldwide do not have electricity in their homes</a>,
            many of them located in Africa and Asia. Information on location and features of energy Infrastructure can inform policy makers in energy access planning and is critical to deploying energy resources. Such information will allow energy developers to understand how to extend the network to unattended communities. </p>
          </div>
         </div> 
        <div class="row">
        <div class="col-md-12">
          <img src="img/motivation/Global Access to Electricity.png" class="img-responsive" alt=""style="width:100%; height:70%">
          <caption><i>Images from: <a href="https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/">
            https://www.visualcapitalist.com/mapped-billion-people-without-access-to-electricity/</a></i></caption>
        
            <p style="margin-top: 2.5em" class="text-muted"  style="text-align: justify">However, energy data for developers is often static, meaning that it is outdated, incomplete or inaccessible. This is a common issue for developers of Distributed Energy Resources (DER) who wants to connect Microgrids into a utilities network. </p> 
            <p class="text-muted"  style="text-align: justify"> One potential solution is to automate the process of mapping energy infrastructure using deep learning. Using deep learning, we can feed an image to a model, and the model is able to make predictions about the contents or characteristics of that image, In our work, given overhead imagery taken from satellites as input, a machine learning model can output the types and locations of energy infrastructures present in the image. The output information on existing energy infrastructures can then help inform energy access planning, such as deciding the most cost-effective option among distributed generation, on-grid and micro-grid electrification strategies.</p>
          </div>
        </div>
    </div>


        <!--<div class="col-md-6">
          <h3 class="service-heading">Deep Learning</h3>
          <p class="text-muted" style="text-align: justify;">
            Using deep learning, we can feed an image to a model, and the model is able to make predictions about the contents or characteristics of that image. A common
            technique for image analysis is classification, in which the model predicts the class of the image out of a list of possible classes.
            In the image on the right, the model predicts that the input picture is of a cat, and is 98% confident with this classification. The model learns how to predict
            these classifications based on examples that are shown to it. After it has been trained, it can classify images that it has not seen before.
          </p>
        </div>
        
        <div class="col-md-6">
          <img src="img/motivation/cat_image_classifier.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Images from: <a href="https://www.codeproject.com/Articles/5160467/Image-Classification-Using-Neural-Networks-in-NET">
            Code Project</a></i></caption>
        </div>-->
     
     
     <div class="row text-center">
      <h3 class="service-heading">Object Detection</h3>
        <div class="row">
          
          <div class="col-md-6">
          <img src="img/motivation/object detection explanation.jpeg" class="img-responsive" alt="" style="width:90%">
          <caption><i>Images from: <a href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">
            You only look once, or YOLO</a></i></caption>
          </div>
          
          
        <div class="col-md-6">
          <p class="text-muted" style="text-align: justify;">
            For this project, we are focusing on object detection, which is a combination of classification with localization. The model analyzes
            images and predicts bounding boxes that surround each object. It then also classifies each object, producing a confidence score
            corresponding to the prediction. In the image on the left, the model predicted that there were different objects in the boxes shown
            in green, yellow and pink, and also predicted that the object within these colored boxes were a handbag, a car and a person repectivelly.
            The model learns how to predict these boxes and classifications based on examples shown to it. These examples have labels that we refer
            to as ground truth that contain the information of where the objects are in the image.
          </p>
        </div>
       </div>
      </div>
      
    
      <div class="row text-center">
      <h3 class="service-heading">Applying Deep Learning to Overhead Imagery</h5>
          <div class="row">

            <div class="col-md-12 my-auto">
              <img src="img/motivation/apply deep learning model to overhead imagery.png" class="img-responsive" alt="apply deep learning" style="width:90%">
            </div>
          </div>
          
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                 After training this model, we could apply it to a collection of overhead imagery to locate and classify energy infrastructure across a whole region. 
We start with the training of wind turbines, because wind turbines are relatively homogeneous. Outside of size differences, there aren't a lot of variations between wind turbines. We also start with data from within the US, because there are readily available satellite imagery of wind turbines in continental US. </p>
             </div>
          </div>
        </div>
			

			<div class="row text-center">
			  <h3 class="service-heading">Challenges with Object Detection</h3>

			  <div class="row">
  			  <div class="col-md-6">
              <h4>Problem 1: Lack of labeled data</h4>
              <p class="text-muted" style="text-align: justify;"> Object detection networks, like the one used in this work, are notorious for their "data hunger," requiring
                <strong>large amounts of annotated training data to perform well</strong>.
                For common infrastructure like buildings and roads, there is ample real-world data available to train such models.
                However, energy infrastructure is <strong>rare</strong> both in number and density, so collecting and annotating large amounts of satellite images manually is <strong>expensive</strong> and often not possible. 
              </p>
          </div>
        
          <div class="col-md-6">
              <img src="img/motivation/Data Hungry.png" class="img-responsive" alt=""style="width:80%; height:80%">
          </div>
        </div>
        
        <div class="row">
          <div class="col-md-6">
            <h4>Problem 2: Domain Adaptation</h4>
            <p class="text-muted" style="text-align: justify;"> Object detection models have worse performance on images that are <strong>not similar</strong> to the ones that it has previously seen. In our case, this means the model will struggle when trying to apply it to <strong>new geographies</strong> and to <strong>different variations</strong> of energy infrastructure. 
            </p>
          </div>
        
        <div class="col-md-6">
          <img src="img/motivation/Domain Adaptation.png" class="img-responsive" alt=""style="width:80%; height:80%">
          </div>
        </div>
    </div>
        
     <div class="row text-center">
      <h3 class="service-heading">Solution: Synthetic Imagery</h3>
        <div class="row">
           <div class="col-md-12 my-auto">
         
            <p class="text-muted" style="text-align: justify;">
              Since training data is difficult to collect, in this project we explore creating synthetic data to supplement the real data that are available.
              We do this using CityEngine, which can render and generate 3D landscapes and structures
              based on input from the designer. In our case, we are populating a landscape with wind turbine models.
              Then we can position the camera in the overhead position and capture images with similar appearances to overhead imagery.
              Since we placed the wind turbines in the synthetic image, we can also generate ground truth labels for each of these images.
            </p>
        </div>
        <div class="col-md-12">
          <img src="img/motivation/synthetic imagery.png" class="img-responsive" alt=""style="width:90%; height:90%">
        </div>
      </div>
    </div>

      
           

      <div class="row text-center">
        <div class="col-md-6">
          <h3 class="service-heading">Previous Work</h3>
          <p class="text-muted" style="text-align: justify;">
            For five years, the Duke Energy Data Analytics Lab has worked on developing deep learning models that identify energy infrastructure, with an end goal of
            generating maps of power grid networks that can aid policymakers in implementing effective
            electrification strategies. In 2015-16, researchers created a model that can detect solar photovoltaic arrays with high accuracy <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-2015-2016">[2015-16
                Bass Connections Team]</a>.
            In 2018-19, this model was improved to identify different types of transmission and distribution energy infrastructures,
            including power lines and transmission towers <a
                href="https://bassconnections.duke.edu/project-teams/energy-data-analytics-lab-energy-infrastructure-map-world-through-satellite-data-2018">[2018-19
                Bass Connections Team]</a>. Last year's project focused on increasing the adaptability of detection models
            across different geographies by creating realistic synthetic imagery <a
                href="https://bass-connections-2019.github.io/">[2019-20
            Bass Connections Team]</a>. In our project, we build upon this progress and try to improve the model's ability to detect rare objects.
          </p>
        </div>
        <div class="col-md-6">
          <img src="img/overview_OLD/energy_information.png" class="img-responsive center" alt=""
          style="width:100%">
        </div>
      </div>
      
      
		</div>
	</section>

  <!-- Methodology -->
  <section class="page-section" id="methodology">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Methodology</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;">
                Below you can see a summary of the experiments we ran to explore whether adding synthetic imagery
                improves the performance of an object detection model across geographic domains. We first need to collect 
                real and synthetic imagery, and then we can create two datasets. One dataset contains purely real imagery, 
                while the second contains the same real imagery, but we add in some synthetic images. We can train 
                an object detection model on the first dataset, test its performance, and then do the same with the second 
                dataset, and finally compare the two performances. In this section, we'll walk through each of the steps 
                required to perform this experiment.
              </p>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Overview of Steps.png" class="img-responsive" alt=""style="width:100%">
            </div>
          </div>
          <hr class="divider light my-4" />
          <h2 class="service-heading text-left">Collecting Real Imagery</h2>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> For our overhead imagery of wind turbines, we chose to sample them from
                <a href="https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/">the National Agriculture Imagery Program</a>.
                This imagery covers a large part of the US and is very high resolution, making it great for our experiments.
                We collected imagery in three different regions that we called Northwest, Northeast, and Eastern Midwest. We noticed
                differences in the visual appearance of the background in the images collected in these three regions:
              </p>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly brown</dd>
                <dd class="text-muted">- Mostly desert and grassland</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Northeast</h5>
              <dl>
                <dd class="text-muted">- Hue is very green</dd>
                <dd class="text-muted">- Mostly forests</dd>
              </dl>
            </div>
            <div class="col-md-4 my-auto">
              <h5 class="service-heading">Eastern Midwest</h5>
              <dl>
                <dd class="text-muted">- Hue is mostly green, some brown</dd>
                <dd class="text-muted">- Primarily farmland</dd>
              </dl>
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/methodology/Collecting Overhead Imagery.png" class="img-responsive" alt="" style="width:80%"><br>
              <caption><i>Figure: Each dot represents a single image that we collected.</i></caption>
            </div>
          </div>
          <br>
          <br>
          <div class="row">
            <div class="col-md-12 my-auto">
              <p class="text-muted" style="text-align: justify;"> Below we can see the regions splits by which states they include,
                as well as the number of images we collected for each region.
              </p>
              <img src="img/methodology/Where Our Data is From.png" class="img-responsive" alt="" style="width:70%"><br>
              <caption><i>Figure: Map of the U.S. showing the U.S. states and number of images we collected in each region</i></caption>
            </div>
          </div>
        </div>
      </div>


      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Creating Synthetic Imagery</h2>
          <p class="text-muted" style="text-align: left;"> To create synthetic imagery, we use a software called CityEngine. As inputs to
            this process, we need a list of background images that do not contain any wind turbines and a 3D model of a wind turbine. We then can automatically generate synthetic images. 
            First, the software places a randomly chosen background image and then randomly generates 3D wind turbine models
            on top of the background image to create the 3D scene shown in the middle of the figure below. Next, the software moves a simulated camera 
            in the overhead/bird's eye view and save these overhead images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Synthetic Generation Process.png" class="img-responsive" alt="" style="width:100%">
        </div>
      </div>

      <br><br>
      <div class="row">
        <div class="col-md-4 my-auto">
          <p class="text-muted" style="text-align: justify;"> We can repeat this process but remove the background images, and color
            in the turbine models as black to retrieve information on where the turbine models are located. The black pixels in these
            images can be automatically grouped together to locate the wind turbines and create a formatted label that contains the 
            bounding box around each turbine model.
          </p>
        </div>
        <div class="col-md-8 my-auto">
          <img src="img/methodology/Generating Annotations.png" class="img-responsive" alt="" style="width:100%">
          <caption><i>Figure: Side-by-side of an RGB image with its corresponding black-and-white label.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <p class="text-muted" style="text-align: justify;"> These synthetic images are simple, cheap, and fast to create. All we need are
            images for the background, and unlabeled imagery is far easier to acquire than labeled imagery. The rest of the process of generating the images and the labels is done automatically, making this a great
            alternative when we do not have enough real imagery or when it is time-consuming or expensive to collect. Below are some example
            synthetic images created from a variety of background images.
          </p>
        </div>
        <div class="col-md-12 my-auto">
          <img src="img/methodology/Example Synthetic Images.png" class="img-responsive" alt="" style="width:80%;"><br>
          <caption class="text-muted"><i>Figure: Our synthetic images contain a variety of background images and camera angles. They look fairly similar to our real imagery.</i></caption>
        </div>
      </div>
      <br><br>
      <div class="row">
        <div class="col-md-12 my-auto">
          <h3 class="service-heading">Synthetic Imagery Design Considerations</h3>
          <p class="text-muted" style="text-align: left;"> The design of the synthetic imagery is important because the closer the
            synthetic imagery is to the real imagery, the more the synthetic imagery will improve our performance when adding it to our
            training set.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Turbine Size Distribution.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Size of the Synthetic Turbines</h5>
          <p class="text-muted" style="text-align: left;"> The first consideration we have to make is what size to make the
            synthetic turbines. For this issue, we chose to model the size distribution of the synthetic turbines after the size
            distribution of the real turbines. We created a histogram of the size of the turbines in our real imagery,
            and modeled this in our synthetic imagery with multiple bins of uniform distributions.
          </p>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Angle of the Camera</h5>
          <p class="text-muted" style="text-align: left;"> The next decision we had to make about the synthetic imagery design was
            the angle of the simulated camera when we are capturing photos. We noticed that in the real imagery,
            some of the images were captured at an angle. In the real overhead imagery, you can see the pole of
            the turbine due to the angle of the camera. We observed that about half of the real images were taken from directly above (90 degrees), 
            and the rest were taken at a variety of angles that were between 60-90 degrees. In our synthetic image generation process, we 
            chose half of the time to take the image from directly above. The other half of the time, we would use a 
            randomly chosen camera angle between 60 and 90 degrees.
          </p>
        </div>
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Angle of Camera 2.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Bounding box size distribution of turbines in real imagery.</i></caption>
        </div>
      </div>
      <br>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/methodology/Nearby Background Image.png" class="img-responsive" alt="" style="width:100%">
          <caption class="text-muted"><i>Figure: Test image and nearby collected background image.</i></caption>
        </div>
        <div class="col-md-6 my-auto">
          <h5 class="service-heading">Which Background Images to Use</h5>
          <p class="text-muted" style="text-align: left;"> We also have to choose which background images to have placed under our
            synthetic wind turbine models. We chose to collect background imagery close to the real images in our testing set so that our synthetic
            imagery would look as close as possible to the target data. We may likely be able to collect unlabelled imagery from around the region we wish to test 
            on for use as background images. Using the background images close to our testing locations allows us to estimate the potential performance 
            increase that the synthetic data can provide without adding in confounding variables such as a mismatch between the synthetic background 
            image domain and the target domain.
          </p>
        </div>
      </div>

      <hr class="divider light my-4" />

      <div class="row">
        <div class="col-md-12 my-auto">
        <h2 class="service-heading">Constructing the Datasets</h2>

        <div class="row">
          <div class="col-md-12 my-auto">
            <h5 class="service-heading">Clustering and Stratified Sampling</h5>
              <p class="text-muted" style="text-align: justify;">
                Because our self-defined geographic regions are wide and diverse, it's important for our training and testing datasets
                to be representative of a given geographic region. To increase homogeneity within a region, we used DBSCAN (Density-based spatial clustering of applications with noise) for clustering
                within each region and then stratified sample from each cluster to construct our 100:100 (train to test ratio) constrained
                baseline datasets. Each point represents an image, and each color represents a cluster or strata. Points that are in the same cluster are more similar to each other than points that are in different stratas. </p>
          </div>
          <div class="col-md-12 my-auto">
            <p style="text-align:center;">
            <img src="img/methodology/Clustering and Stratified Sampling.png" class="img-responsive" alt="" class="center" style="width:80%; height:70%">
            </p>
            <br>
            <caption class="text-muted"><i>Figure: DBSCAN clustering within each geographic region and then stratified random sampling from each cluster to construct our training and testing datasets. Each dot represents a real imge, and each color represents a different strata or cluster. Because our training and testing datasets contain 100 images each, and there are 4 clusters within the Northeast region shown here, we randomly sample 25 images from each cluster for our train data, and a different set of 25 images from each cluster for our test data. </i></caption>
          </div>
        </div>

          <h5 class="service-heading">Optimizing Real to Synthetic Ratio</h5>
          <div class="row">

            <div class="col-md-6 my-auto">
              <img src="img/methodology/Ratio Test Experimental Design.png" class="img-responsive" alt="Ratio Test Experimental Design" style="width:100%">
            </div>

            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;">
                In order to construct our baseline and adding synthetic datasets, we need to figure out what real to synthetic ratio yields
                the largest gain in performance. If we add too much synthetic data, then we run the risk of overfitting to synthetic data. If we add too little, then it won't change performance significantly. To find this ratio, we designed the following experiment, where we test 1:0, 1:0.5, 1:0.75,
                1:1, and 1:2 real to synthetic ratios. After conducting these experiments, we found that 1:0.75 yields the largest performance
                boost. Therefore, we design our experiments using the 1:0.75 ratio.  </p>
             </div>
        </div>

      <hr class="divider light my-4" />
      <div class="row">
        <div class="col-md-12 my-auto">
          <h2 class="service-heading">Experimental Setup</h2>
          <br>
          <p class="text-muted" style="text-align: justify;">
            Having clustered and sampled our data and found the optimal real to synthetic ratio, our final datasets for each region is: </p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Baseline: </b> Train on 100 Real Non-Target Images, Test on 100 Target Domain Images</li>
                <li class="text-muted" style="text-align: justify;"><b>Modified: </b> Train on 100 Real Non-Target Images + 75 Syn Target Images, Test on 100 Target Domain Images </li>
              </ul>
          </br>

          <h5 class="service-heading">Overview</h5>
          <div class="row">
            <div class="col-md-6 my-auto">
              <p class="text-muted" style="text-align: justify;">The long-term goal of our project is to apply our object detection model to a variety of unseen domains around the world, especially
                in developing areas where information on energy infrastructure is hard to obtain. This requires our model to be able to <strong>generalizable</strong>,
                that is, the ability to perform well on images that are different from the images it was trained on. Our hypothesis is that synthetic
                imagery will help our model generalize to unseen domain - the model performance will improve when testing on a new geography in the presence
                of synthetic data. To test our hypothesis, we designed cross-domain experiments, where we train on the source domain, but we test on the
                target domain. To see the effects of synthetic imagery, we add synthetic imagery from the target domain in our modified datasets. We also
                conducted within domain experiments, where we train and test on the target domain, to establish baseline performance. </p>
             </div>
            <div class="col-md-6 my-auto">
              <img src="img/methodology/Overall Experimental Setup.png" class="img-responsive" alt="" style="width:100%; height:100%">
            </div>

           <div class="col-md-6 my-auto">
             <img src="img/methodology/Pairwise Cycle.png" class="img-responsive" alt="Pairwise Experiment Setup" style="width:100%">
            </div>

            <div class="col-md-6 my-auto">
             <p class="text-muted" style="text-align: justify;">
               In our case, each domain is a geographic region that we defined. Namely, Northwest, Northeast, and Eastern Midwest. To test the impact of each region, we run a 3-way pairwise experiment, where we train and test on each of the 3 regions, resulting in 6 experiments. </p>
            </div>
          </div>


          <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Within Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: Within-domain experiment example using Northwest as target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Within Domain Experiment (Target = Source)</h5>
             <p class="text-muted" style="text-align: left;">
               Before we test how well synthetic imagery improves performance when the model is testing on a different domain than the one
               it was trained on, we need to establish some baseline to test if adding synthetic imagery improve our model accuracy. We need to know how well the model is doing when it is trained and
               tested on the same domain. Using Northwest region as our target domain, we ran our baseline and modified experiments,
               and we repeat for the other domains/regions.</p>
          </div>
        </div>
        <br>


        <br>
          <div class="row">
            <div class="col-md-6 my-auto">
            <img src="img/methodology/Cross Domain Experiment.png" class="img-responsive" alt="" style="width:100%">
           <caption class="text-muted"><i>Figure: Cross-domain experiment example using Northwest as target domain and Eastern Midwest as non-target domain.</i></caption>
            </div>
          <div class="col-md-6 my-auto">
            <h5 class="service-heading">Cross Domain Experiment (Target $\neq$ Source)</h5>
             <p class="text-muted" style="text-align: left;">
               We are curious if synthetic data can help overcome differences in geography. To test whether adding synthetic imagery improve our model generalizability, we trained on one region, and tested
               on another region (cross-domain). Then we added synthetic imagery similar to the target domain and compared performance against the baseline.
               Here we demonstrate this experiment setup using Northwest as the target domain and Eastern Midwest as the non-target domain.
               We repeated for the other domains as well. </p>
          </div>
        </div>
        <br>

       </div>
      </div>

    </div>
  </section>

  <!-- Results-->
  <section class="page-section" id="results">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Results</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-6 my-auto">
              <br>
              <img src="img/results/metrics.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">
                Figure 1: The model predicted that 4 objects were wind turbines. 2 of those predictions were correct, meaning
                the precision would be 2/4. There are 3 wind turbines in the image and the model found 2 of these, meaning the
                recall would be 2/3.</font></caption>
            </div>
            <div class = "col-md-6">
              <h3>Performance Metrics</h3>
              <p class="text-muted" style="text-align: justify;">
                To understand our results, it's important that we first understand the metrics that we have chosen to measure
                performance. The primary metrics we will use is Average Precision for an IoU threshold of 0.5 (AP@0.5). We will explain
                the implication of this metric starting with the images on the left.</p>
              <ul>
                <li class="text-muted" style="text-align: justify;"><b>Precision: </b> Out of the objects that the model classified as a wind turbine,
                  what fraction of these were actually wind turbines.</li>
                <li class="text-muted" style="text-align: justify;"><b>Recall: </b> Out of wind turbines present in the data, what fraction of these
                  did the model find.</li>
              </ul>
            </div>
            <div class="row">
              <div class = "col-md-6 my-auto">
                <p class="text-muted" style="text-align: justify;">
                  Now we plot the values of precision and recall a model can reach when doing predictions on a graph, which is known as a precision-recall
                  curve. You can see on the curve that as precision increases, recall decreases, and vice versa. There is hence a tradeoff between
                  precision and recall. However, we would like to have high values for both precision and recall, which means we would like the area
                  under the precision-recall curve to be as high as possible. A metric that quantifies this area is Average Precision (AP). </p>
                <p class="text-muted" style="text-align: justify;">
                  Note that in the machine learning space, a small absolute increase in AP is already a significant improvement.
                </p>
              </div>
              <div class="col-md-6">
                <br>
                <img src="img/results/sample_pr.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 2: Sample Precision Recall Curve. We would like the curve to move to right as much as possible as indicated
                  by the green arrow. </font></caption>
                <br>
                <br>
                <br>
              </div>
              <div class="row">
                <div class="col-md-6 my-auto">
                  <br>
                  <img src="img/results/variance.png" class="img-responsive" alt="" style="width:100%">
                  <br>
                  <caption><font size="2">Figure 3: Sample PR curves of 4 runs of the same experiment. </font></caption>
                  <br>
                  <br>
                </div>
                <div class = "col-md-6">
                  <h3>Reducing Variance</h3>
                  <p class="text-muted" style="text-align: justify;">
                    Due to randomness in machine learning, there will be slight variations between the results of each run, as shown on the left image.
                    Each experiment is therefore repeated 4 times to account for this randomness and improve the accuracy of the result. The average AP
                    value is calculated and used to compare results of our baseline model and model with added synthetic images.</p>
                </div>
          <br>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results</h2>
          <div class="row">
              <div class="col-md-12 text-center">
                <p class="text-muted" style="text-align: justify;">
                  The performances of the model with added synthetic images <b>improve significantly </b> in both within-domain and cross-domain settings.
                  Synthetic images are especially helpful in cross-domain settings, which means they can be useful when there is a lack of data or when it
                  is cost-prohibitive to collect data of the target domain.
                </p>
              </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results.png" class="img-responsive" alt="" style="width:80%">
            </div>
            <div class="col-md-12 my-auto">
              <img src="img/results/overall-results-table.png" class="img-responsive" alt="" style="width:70%">
              <br>
              <caption><font size="2">Figure 4: All values are in AP@0.5. </font></caption>
            </div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-6">
              <img src="img/results/NE-EM-baseline.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 5: Sample test image from the baseline experiment of training on Northeast and testing on Eastern Midwest. The model falsely predicted an architecture to the right as 3 wind turbines. </font></caption>
            </div>
            <div class="col-md-6">
              <img src="img/results/NE-EM-adding-synthetic.png" class="img-responsive" alt="" style="width:80%">
              <br>
              <caption><font size="2">Figure 6: Sample test image from the adding synthetic experiment of training on Northeast and testing on Eastern Midwest. The adding synthetic prediction is able to achieve a higher precision. </font></caption>
              <br>
              <br>
            </div>
            <div class="col-md-1"></div>
          </div>
          <hr class="divider light my-4" />
          <h2 style="text-align: left;">Results of Each Geographic Domain Respectively</h2>
          <div class="row">
            <div class="col-md-12 text-center">
              <p class="text-muted" style="text-align: justify;">
                Here we will present a closer look into the results of training with real images from each of the 3 geographic regions respectively.
                There is a disparity in performance when the model is trained with real images of different geographic domains. In particular, in
                cross-domain experiments that test on Eastern Midwest, the model perform generally worse than when valiating on other regions.
                Meanwhile, the model benefits the most from the addition of synthetic imagery if the baseline model does not perform comparatively
                well, such as in the case of Eastern Midwest.
              </p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
              <img src="img/results/results-NE.png" class="img-responsive" alt="" style="width:100%">
              <img src="img/results/results-NE-table.png" class="img-responsive" alt="" style="width:100%">
              <br>
              <caption><font size="2">Figure 7: Results of training with real images from Northeast. </font></caption>
              <br>
              <br>
              <br>
              <br>
            </div>
            <div class="col-md-6 my-auto">
                <img src="img/results/results-NW.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-NW-table.png" class="img-responsive" alt="" style="width:100%">
                <br>
                <caption><font size="2">Figure 8: Results of training with real images from Northwest. </font></caption>
                <br>
                <br>
                <br>
                <br>
            </div>
          </div>
          <div class="row">
            <div class="col-md-6 my-auto">
                <img src="img/results/results-EM.png" class="img-responsive" alt="" style="width:100%">
                <img src="img/results/results-EM-table.png" class="img-responsive" alt="" style="width:100%">
                <caption><i>Figure 9: Results of training with real images from Eastern Midwest</i></caption>
            </div>
            <div class="col-md-6">
                <p class="text-muted" style="text-align: justify;">
                  As shown above, the model performs consistently worse when testing on Eastern Midwest, yet that is when the model
                  has the greatest improvement in AP from the addition of synthetic imagery. </p>
                <br>
                <p class="text-muted" style="text-align: justify;">
                  One explanation for the disparity from performance in Eastern Midwest is the wide diversity of geographic backgrounds
                  presented in the region's dataset. The exact model performance and improvement synthetic images can make depend on geographic
                  regions. Regions with diverse backgrounds may limit the model's performance. </p>
            </div>
          </div>
  </section>


  <!-- Key Takeaways-->
  <section class="page-section" id="key-takeaways">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Key Takeaways</h2>

          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-1 my-auto"></div>
            <div class="col-md-10 my-auto">
              <p class="text-muted" style="text-align: justify;">The results show that adding the curated synthetic imagery improves the performance of
              our object detection model in all cases. This is especially the case when we have a cross domain setting (testing on an unseen region).
              The performance increase is more limited in the within domain setting, where there the model is testing on a previously seen region. We
              also explored the various ratios of real to synthetic imagery and found an optimal ratio. This method of synthetic generation is cheap
              and fast to produce, allowing us to help an object detection model perform on new domains or when we lack training data, which is often the
              case when we are trying to obtain information on energy infrastructure. With the aid of synthetic imagery, this method of collecting
              locations of energy infrastructure could fill in the information gaps that energy access planners need when making decisions about electrification.</p>
            </div>
            <div class="col-md-1 my-auto"></div>
          </div>
          <br>
          <div class="row">
            <div class="col-md-12">
              <img src="img/key_takeaways/Overall Results.png" class="img-responsive" alt="" style="width:50%">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Future Work -->
  <section class="page-section" id="future-work">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Future Work</h2>
          <hr class="divider light my-4" />
          <div class="row">
            <div class="col-md-12 my-auto">
              <ol class="text-muted" style="text-align: justify;">
                <li>
                  Apply these techniques to <strong>detect other types of energy infrastructure</strong>. Because high voltage transmission towers are
                  similar structure to wind turbines, we can easily adapt our synthetic image generation process for transmission towers. We could then
                  test this synthetic imagery in the same manner as what we performed for the synthetic imagery of wind turbines, and see if this method
                  extends to this other energy infrastructure.
                </li>
                <br>
                  <div class="figure-img">
                    <img src="img/future_work/High_voltage_transmission_towers_and_lines.jpg" class="img-responsive" alt="" title="Img Source: https://upload.wikimedia.org/wikipedia/commons/b/b4/High_voltage_transmission_towers_and_lines.jpg" style="width:50%">
                  </div>
                <br>
                <li>
                  Try few shot learning, where we use <strong>small amounts of real images and large amounts of synthetic data</strong> to adapt our object detection
                  model to any region that we choose.
                </li>
                <br>
                <div class="figure-img">
                    <img src="img/future_work/Few Shot Learning.png" class="img-responsive" alt="" title="Source: https://deepai.org/publication/a-new-benchmark-for-evaluation-of-cross-domain-few-shot-learning" style="width:100%">
                </div>
                <br>
                <li>
                  We would also like to explore <strong>more methods of generating synthetic data</strong>. One method in particular would be using
                  generative adversarial networks (GANs) to generate synthetic imagery with deep learning. This could look more realistic
                  than the current synthetic imagery, since the synthetic image generation process would learn from the real imagery we have.
                </li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Acknowledgements -->
  <section class="bg-light page-section" id="acknowledgements">
    <div class="container">
      <div class="row">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading text-uppercase">Acknowledgements</h2>
        </div>
      </div>
      <br>
      <div class="row justify-content-center">
        <p class="text-muted" style="text-align: justify;"> We would like to thank Dr. Kyle Bradbury, Dr. Jordan Malof, and Wayne Hu for their
          help and guidance along the way. We would also like to thank the previous Bass Connections and Data+ teams for their work leading up
          to this project. We would also like to thank Dr. Rob Fetter, Dr. Marc Jeuland, and Dr. Luana Lima for sharing their work with us.
          Thank you to to the Duke Bass Connections Program and Energy Initiative that supported this project.
        </p>
      </div>
      <div class="row">
        <div class="col-md-6 my-auto">
          <img src="img/acknowledgement/Bass-Connections-Logo.jpeg" class="img-responsive" alt="" style="width:100%">
        </div>

        <div class="col-md-6 my-auto">
          <img src="img/acknowledgement/Duke Energy Initiative Logo.png" class="img-responsive" alt="" style="width:100%">
        </div>
       </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="row align-items-center">
        <div class="col-md-6">
          <span class="copyright"></span>
        </div>
        <div class="col-md-6">
          <ul class="list-inline social-buttons">
            <li class="list-inline-item">
              <a href="https://github.com/Duke-BC-DL-for-Energy-Infrastructure">
                <i class="fab fa-github"></i>
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Contact form JavaScript -->
  <script src="js/jqBootstrapValidation.js"></script>
  <script src="js/contact_me.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/agency.min.js"></script>

</body>

</html>
